{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Harvester Overview \u00b6 Harvester is an open-source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open alternative to using a proprietary HCI stack that incorporates the design and ethos of Cloud Native Computing . Harvester Features \u00b6 Harvester implements HCI on bare metal servers. Harvester is designed to use local, direct attached storage instead of complex external SANs. It ships as an integrated bootable appliance image that can be deployed directly to servers through an ISO or PXE boot artifact. Some notable features of Harvester include the following: VM lifecycle management including SSH-Key injection, cloud-init, and graphic and serial port console VM live migration support Supported VM backup and restore Distributed block storage Multiple network interface controllers (NICs) in the VM connecting to the management network or VLANs Virtual Machine and cloud-init templates Rancher integration with multi-cluster management and the Harvester node driver PXE/iPXE boot support Virtual IP and bond NIC support Monitoring integration Harvester Architecture \u00b6 The following diagram outlines a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. KubeVirt is a virtual machine management add-on for Kubernetes. Elemental for openSUSE Leap 15.3 is a Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster. Hardware Requirements \u00b6 To get the Harvester server up and running, the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory 32 GB minimum; 64 GB or above preferred Disk Capacity 140 GB minimum; 500 GB or above preferred Disk Performance 5,000+ random IOPS per disk (SSD/NVMe). Management nodes (first three nodes) must be fast enough for etcd . Network Card 1 Gbps Ethernet minimum; 10Gbps Ethernet recommended Network Switch Trunking of ports required for VLAN support Quick start \u00b6 You can install Harvester via ISO installation or PXE Boot Installation. Instructions are provided in sections below. ISO Installation \u00b6 You can use the ISO to install Harvester directly on the bare metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases . During the installation, you can either choose to form a new cluster or join the node to an existing cluster. Note: This video shows a brief overview of the ISO installation process. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster or by joining an existing one. Choose the installation device to which the Harvester cluster will be formatted. Configure the hostname and select the network interface for the management network. By default, Harvester will create a bond NIC named harvester-mgmt , and the IP address can either be configured via DHCP or by static method. Optional: Configure the DNS servers; use commas as delimiters. Configure the Virtual IP with which you can use to access the cluster or join other nodes to the cluster. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default ssh user is rancher . Optional: Configure the NTP Servers of the node if needed. Default is 0.suse.pool.ntp.org . Optional: If you need to use an HTTP proxy to access the outside world, enter the proxy URL address; otherwise, leave this blank. Optional: You can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . Optional: If you need to customize the host with cloud-init configuration, enter the HTTP URL. Confirm the installation options and Harvester will be installed to your host. The installation may take a few minutes to complete. Once the installation is complete, the host will restart, and a console UI with management URL and status will be displayed. (You can Use F12 to switch between the Harvester console and the Shell). The default URL of the web interface is https://your-virtual-ip . Users will be prompted to set the password for the default admin user at first login. Other Installation Methods \u00b6 Harvester can be installed automatically also. Please refer to PXE Boot Install for detailed instructions for additional guidance. More iPXE usage examples are available at harvester/ipxe-examples .","title":"Harvester Overview"},{"location":"#harvester-overview","text":"Harvester is an open-source hyper-converged infrastructure (HCI) software built on Kubernetes. It is an open alternative to using a proprietary HCI stack that incorporates the design and ethos of Cloud Native Computing .","title":"Harvester Overview"},{"location":"#harvester-features","text":"Harvester implements HCI on bare metal servers. Harvester is designed to use local, direct attached storage instead of complex external SANs. It ships as an integrated bootable appliance image that can be deployed directly to servers through an ISO or PXE boot artifact. Some notable features of Harvester include the following: VM lifecycle management including SSH-Key injection, cloud-init, and graphic and serial port console VM live migration support Supported VM backup and restore Distributed block storage Multiple network interface controllers (NICs) in the VM connecting to the management network or VLANs Virtual Machine and cloud-init templates Rancher integration with multi-cluster management and the Harvester node driver PXE/iPXE boot support Virtual IP and bond NIC support Monitoring integration","title":"Harvester Features"},{"location":"#harvester-architecture","text":"The following diagram outlines a high-level architecture of Harvester: Longhorn is a lightweight, reliable and easy-to-use distributed block storage system for Kubernetes. KubeVirt is a virtual machine management add-on for Kubernetes. Elemental for openSUSE Leap 15.3 is a Linux distribution designed to remove as much OS maintenance as possible in a Kubernetes cluster.","title":"Harvester Architecture"},{"location":"#hardware-requirements","text":"To get the Harvester server up and running, the following minimum hardware is required: Type Requirements CPU x86_64 only. Hardware-assisted virtualization is required. 8-core processor minimum; 16-core or above preferred Memory 32 GB minimum; 64 GB or above preferred Disk Capacity 140 GB minimum; 500 GB or above preferred Disk Performance 5,000+ random IOPS per disk (SSD/NVMe). Management nodes (first three nodes) must be fast enough for etcd . Network Card 1 Gbps Ethernet minimum; 10Gbps Ethernet recommended Network Switch Trunking of ports required for VLAN support","title":"Hardware Requirements"},{"location":"#quick-start","text":"You can install Harvester via ISO installation or PXE Boot Installation. Instructions are provided in sections below.","title":"Quick start"},{"location":"#iso-installation","text":"You can use the ISO to install Harvester directly on the bare metal server to form a Harvester cluster. Users can add one or many compute nodes to join the existing cluster. To get the Harvester ISO, download it from the Github releases . During the installation, you can either choose to form a new cluster or join the node to an existing cluster. Note: This video shows a brief overview of the ISO installation process. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer . Choose the installation mode by either creating a new Harvester cluster or by joining an existing one. Choose the installation device to which the Harvester cluster will be formatted. Configure the hostname and select the network interface for the management network. By default, Harvester will create a bond NIC named harvester-mgmt , and the IP address can either be configured via DHCP or by static method. Optional: Configure the DNS servers; use commas as delimiters. Configure the Virtual IP with which you can use to access the cluster or join other nodes to the cluster. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default ssh user is rancher . Optional: Configure the NTP Servers of the node if needed. Default is 0.suse.pool.ntp.org . Optional: If you need to use an HTTP proxy to access the outside world, enter the proxy URL address; otherwise, leave this blank. Optional: You can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . Optional: If you need to customize the host with cloud-init configuration, enter the HTTP URL. Confirm the installation options and Harvester will be installed to your host. The installation may take a few minutes to complete. Once the installation is complete, the host will restart, and a console UI with management URL and status will be displayed. (You can Use F12 to switch between the Harvester console and the Shell). The default URL of the web interface is https://your-virtual-ip . Users will be prompted to set the password for the default admin user at first login.","title":"ISO Installation"},{"location":"#other-installation-methods","text":"Harvester can be installed automatically also. Please refer to PXE Boot Install for detailed instructions for additional guidance. More iPXE usage examples are available at harvester/ipxe-examples .","title":"Other Installation Methods"},{"location":"authentication/","text":"Authentication \u00b6 After installation, user will be prompted to set the password for the default admin user on the first-time login. Note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","title":"Authentication"},{"location":"authentication/#authentication","text":"After installation, user will be prompted to set the password for the default admin user on the first-time login. Note In the single cluster mode, only one default admin user is provided. Check out the Rancher Integration for multi-tenant management.","title":"Authentication"},{"location":"faq/","text":"FAQ \u00b6 This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester. How can I ssh login to the Harvester node? $ ssh rancher@node-ip What is the default login username and password of the Harvester dashboard? username: admin password: # you will be promoted to set the default password when logging in for the first time How can I access the kubeconfig file of the Harvester cluster? # You can either download the kubeconfig file from the Harvester # dashboard or access it via one of the Harvester management nodes. E.g., $ cat /etc/rancher/rke2/rke2.yaml How do I access the embedded Rancher dashboard? Please refer to the troubleshooting section . How to install the qemu-guest-agent of a running VM. # cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/topics/cli.html#clean","title":"FAQ"},{"location":"faq/#faq","text":"This FAQ is a work in progress designed to answer the questions our users most frequently ask about Harvester. How can I ssh login to the Harvester node? $ ssh rancher@node-ip What is the default login username and password of the Harvester dashboard? username: admin password: # you will be promoted to set the default password when logging in for the first time How can I access the kubeconfig file of the Harvester cluster? # You can either download the kubeconfig file from the Harvester # dashboard or access it via one of the Harvester management nodes. E.g., $ cat /etc/rancher/rke2/rke2.yaml How do I access the embedded Rancher dashboard? Please refer to the troubleshooting section . How to install the qemu-guest-agent of a running VM. # cloud-init will only be executed once, reboot it after add the cloud-init config with the following command. $ cloud-init clean --logs --reboot https://cloudinit.readthedocs.io/en/latest/topics/cli.html#clean","title":"FAQ"},{"location":"upgrade/","text":"Upgrading Harvester \u00b6 Note Upgrade is not supported from previous versions to v0.3.0 version. A manual upgrade process starting with v0.3.0 is being investigated. Harvester will inform the community once this process is in place. One-click upgrade will be supported starting with the v1.0.0 release.","title":"Upgrading Harvester"},{"location":"upgrade/#upgrading-harvester","text":"Note Upgrade is not supported from previous versions to v0.3.0 version. A manual upgrade process starting with v0.3.0 is being investigated. Harvester will inform the community once this process is in place. One-click upgrade will be supported starting with the v1.0.0 release.","title":"Upgrading Harvester"},{"location":"upload-image/","text":"Upload Images \u00b6 Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes. Upload Images via URL \u00b6 To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. Note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time. Upload Images via Local File \u00b6 Currently, qcow2, raw, and ISO images are supported. Note Please do not refresh the page until the file upload is finished. This feature is temporarily unusable on the single cluster UI and will be fixed via #1415 . Create Images via Volumes \u00b6 On the Volumes page, click Export Image . Enter image name to create image.","title":"Upload Images"},{"location":"upload-image/#upload-images","text":"Currently, there are three ways that are supported to create an image: uploading images via URL, uploading images via local files, and creating images via volumes.","title":"Upload Images"},{"location":"upload-image/#upload-images-via-url","text":"To import virtual machine images in the Images page, enter a URL that can be accessed from the cluster. Description and labels are optional. Note The image name will be auto-filled using the URL address's filename. You can customize the image name at any time.","title":"Upload Images via URL"},{"location":"upload-image/#upload-images-via-local-file","text":"Currently, qcow2, raw, and ISO images are supported. Note Please do not refresh the page until the file upload is finished. This feature is temporarily unusable on the single cluster UI and will be fixed via #1415 .","title":"Upload Images via Local File"},{"location":"upload-image/#create-images-via-volumes","text":"On the Volumes page, click Export Image . Enter image name to create image.","title":"Create Images via Volumes"},{"location":"dev/dev-mode/","text":"Developer Mode Installation \u00b6 Developer mode (dev mode) is intended to be used for testing and development purposes. This video shows the dev mode installation. Requirements \u00b6 Multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD is created. The Kubernetes node must have hardware virtualization support. To validate the support, use this command: cat /proc/cpuinfo | grep vmx - If you are using an RKE cluster, ipv4.ip_forward must be enabled for the CNI plugin in order for pod network to work as expected. Installation \u00b6 Harvester can be installed on a Kubernetes cluster in the following ways: Using the Helm CLI. As a Rancher catalog app, in which case the harvester/harvester repo is added to the Rancher Catalog as a Helm v3 app Please refer to the Harvester Helm chart for more details on installing and configuring the Helm chart. Option 1: Install using Helm \u00b6 Clone the GitHub repository: git clone https://github.com/harvester/harvester.git --depth = 1 Go to the Helm chart: cd harvester/deploy/charts Install the Harvester chart with the following commands: ### To install the chart with the release name `harvester`: ## Create the target namespace kubectl create ns harvester-system ## Install the chart to the target namespace helm install harvester harvester \\ --namespace harvester-system \\ --set longhorn.enabled = true,minio.persistence.storageClass = longhorn Option 2: Install using Rancher \u00b6 Tip You can create a test Kubernetes environment in Rancher using DigitalOcean as cloud provider. For details, see this section . Add the Harvester repo https://github.com/harvester/harvester to your Rancher catalogs by clicking Global > Tools > Catalogs . Specify the URL and name. Set the branch to stable if you need a stable release version. Set the Helm version to be Helm v3 . Click Create . Navigate to your project-level Apps . Click Launch and choose the Harvester app. (Optional) You can modify the configurations if needed. Otherwise, use the default options. Click Launch and wait for the app's components to be ready. Click the /index.html link to navigate to the Harvester UI, as shown in the figure below: DigitalOcean Test Environment \u00b6 You can create a test Kubernetes environment in Rancher using DigitalOcean as a cloud provider, which supports nested virtualization. We recommend using a 8 core, 16 GB RAM droplet, which will have nested virtualization enabled by default. This screenshot shows how to create a Rancher node template that would allow Rancher to provision such a node in DigitalOcean: For more information on how to launch DigitalOcean nodes with Rancher, refer to the Rancher documentation.","title":"Developer Mode Installation"},{"location":"dev/dev-mode/#developer-mode-installation","text":"Developer mode (dev mode) is intended to be used for testing and development purposes. This video shows the dev mode installation.","title":"Developer Mode Installation"},{"location":"dev/dev-mode/#requirements","text":"Multus is installed across your cluster and a corresponding NetworkAttachmentDefinition CRD is created. The Kubernetes node must have hardware virtualization support. To validate the support, use this command: cat /proc/cpuinfo | grep vmx - If you are using an RKE cluster, ipv4.ip_forward must be enabled for the CNI plugin in order for pod network to work as expected.","title":"Requirements"},{"location":"dev/dev-mode/#installation","text":"Harvester can be installed on a Kubernetes cluster in the following ways: Using the Helm CLI. As a Rancher catalog app, in which case the harvester/harvester repo is added to the Rancher Catalog as a Helm v3 app Please refer to the Harvester Helm chart for more details on installing and configuring the Helm chart.","title":"Installation"},{"location":"dev/dev-mode/#option-1-install-using-helm","text":"Clone the GitHub repository: git clone https://github.com/harvester/harvester.git --depth = 1 Go to the Helm chart: cd harvester/deploy/charts Install the Harvester chart with the following commands: ### To install the chart with the release name `harvester`: ## Create the target namespace kubectl create ns harvester-system ## Install the chart to the target namespace helm install harvester harvester \\ --namespace harvester-system \\ --set longhorn.enabled = true,minio.persistence.storageClass = longhorn","title":"Option 1: Install using Helm"},{"location":"dev/dev-mode/#option-2-install-using-rancher","text":"Tip You can create a test Kubernetes environment in Rancher using DigitalOcean as cloud provider. For details, see this section . Add the Harvester repo https://github.com/harvester/harvester to your Rancher catalogs by clicking Global > Tools > Catalogs . Specify the URL and name. Set the branch to stable if you need a stable release version. Set the Helm version to be Helm v3 . Click Create . Navigate to your project-level Apps . Click Launch and choose the Harvester app. (Optional) You can modify the configurations if needed. Otherwise, use the default options. Click Launch and wait for the app's components to be ready. Click the /index.html link to navigate to the Harvester UI, as shown in the figure below:","title":"Option 2: Install using Rancher"},{"location":"dev/dev-mode/#digitalocean-test-environment","text":"You can create a test Kubernetes environment in Rancher using DigitalOcean as a cloud provider, which supports nested virtualization. We recommend using a 8 core, 16 GB RAM droplet, which will have nested virtualization enabled by default. This screenshot shows how to create a Rancher node template that would allow Rancher to provision such a node in DigitalOcean: For more information on how to launch DigitalOcean nodes with Rancher, refer to the Rancher documentation.","title":"DigitalOcean Test Environment"},{"location":"host/host/","text":"Host Management \u00b6 Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are more than three nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. Note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes. Node Maintenance \u00b6 For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature. Cordoning a Node \u00b6 Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you\u2019re done, power back on and make the node schedulable again by uncordoning it. Multi-disk Management - Tech Preview \u00b6 Users can view and add multiple disks as additional data volumes from the host detail page. Go to the Hosts page. On the node you want to modify, click \u22ee > Edit Config . Select the Disks tab and click Add Disks . Select either an additional raw block device or partition to add as an additional data volume. The Force Formatted option is required when adding an entire raw block device to form a single root disk partition using the ext4 filesystem. The Force Formatted option is optional when adding partitions where the filesystem type is ext4 , XFS or cannot be found. It is required when adding partitions of any other filesystem type.","title":"Host Management"},{"location":"host/host/#host-management","text":"Users can view and manage Harvester nodes from the host page. The first node always defaults to be a management node of the cluster. When there are more than three nodes, the two other nodes that first joined are automatically promoted to management nodes to form a HA cluster. Note Because Harvester is built on top of Kubernetes and uses etcd as its database, the maximum node fault toleration is one when there are three management nodes.","title":"Host Management"},{"location":"host/host/#node-maintenance","text":"For admin users, you can click Enable Maintenance Mode to evict all VMs from a node automatically. It will leverage the VM live migration feature to migrate all VMs to other nodes automatically. Note that at least two active nodes are required to use this feature.","title":"Node Maintenance"},{"location":"host/host/#cordoning-a-node","text":"Cordoning a node marks it as unschedulable. This feature is useful for performing short tasks on the node during small maintenance windows, like reboots, upgrades, or decommissions. When you\u2019re done, power back on and make the node schedulable again by uncordoning it.","title":"Cordoning a Node"},{"location":"host/host/#multi-disk-management-tech-preview","text":"Users can view and add multiple disks as additional data volumes from the host detail page. Go to the Hosts page. On the node you want to modify, click \u22ee > Edit Config . Select the Disks tab and click Add Disks . Select either an additional raw block device or partition to add as an additional data volume. The Force Formatted option is required when adding an entire raw block device to form a single root disk partition using the ext4 filesystem. The Force Formatted option is optional when adding partitions where the filesystem type is ext4 , XFS or cannot be found. It is required when adding partitions of any other filesystem type.","title":"Multi-disk Management - Tech Preview"},{"location":"install/harvester-configuration/","text":"Harvester Configuration \u00b6 Configuration Example \u00b6 Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: server_url : https://someserver:8443 token : TOKEN_VALUE os : ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files : - encoding : \"\" content : test content owner : root path : /etc/test.txt permissions : '0755' hostname : myhost modules : - kvm - nvme sysctls : kernel.printk : \"4 4 1 7\" kernel.kptr_restrict : \"1\" dns_nameservers : - 8.8.8.8 - 1.1.1.1 ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org password : rancher environment : http_proxy : http://myserver https_proxy : http://myserver install : mode : create networks : harvester-mgmt : interfaces : - name : ens5 default_route : true method : dhcp force_efi : true device : /dev/vda silent : true iso_url : http://myserver/test.iso poweroff : true no_format : true debug : true tty : ttyS0 vip : 10.10.0.19 vip_hw_addr : 52:54:00:ec:0e:0b vip_mode : dhcp Configuration Reference \u00b6 Below is a reference of all configuration keys. Warning Security Risks : The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. server_url \u00b6 Definition \u00b6 The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is. Example \u00b6 server_url : https://someserver:8443 install : mode : join token \u00b6 Definition \u00b6 The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has. Example \u00b6 token : myclustersecret Or a node token token : \"K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4\" os.ssh_authorized_keys \u00b6 Definition \u00b6 A list of SSH authorized keys that should be added to the default user, rancher . SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME} . This is done by downloading the keys from https://github.com/${USERNAME}.keys . Example \u00b6 os : ssh_authorized_keys : - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D\" - \"github:ibuildthecloud\" os.write_files \u00b6 A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: \"\" : content data are written in plain text. In this case, the encoding field can be also omitted. b64 , base64 : content data are base64-encoded. gz , gzip : content data are gzip-compressed. gz+base64 , gzip+base64 , gz+b64 , gzip+b64 : content data are gzip-compressed first and then base64-encoded. Example os : write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner : root:root path : /etc/connman/main.conf permissions : '0644' - content : | # My new /etc/sysconfig/samba file SMDBOPTIONS=\"-D\" path : /etc/sysconfig/samba - content : !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path : /bin/arch permissions : '0555' - content : | 15 * * * * root ship_logs path : /etc/crontab os.hostname \u00b6 Definition \u00b6 Set the system hostname. If the system hostname is supplied via DHCP, then that value will be used here. If this value is not set and one is not supplied via DHCP, then a random hostname will be generated. Example \u00b6 os : hostname : myhostname os.modules \u00b6 Definition \u00b6 A list of kernel modules to be loaded on start. Example \u00b6 os : modules : - kvm - nvme os.sysctls \u00b6 Definition \u00b6 Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf . Values must be specified as strings. Example \u00b6 os : sysctls : kernel.printk : 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict : \"1\" # force the YAML parser to read as a string os.dns_nameservers \u00b6 Definition \u00b6 Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS. Example \u00b6 os : dns_nameservers : - 8.8.8.8 - 1.1.1.1 os.ntp_servers \u00b6 Definition \u00b6 Fallback ntp servers to use if NTP is not configured elsewhere in the OS. Example \u00b6 os : ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org os.password \u00b6 Definition \u00b6 The password for the default user, rancher . By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from /etc/shadow . You can also encrypt a password using openssl passwd -1 . Example \u00b6 Encrypted: os : password : \"$1$tYtghCfK$QHa51MS6MVAcfUKuOzNKt0\" Or clear text: os : password : supersecure os.environment \u00b6 Definition \u00b6 Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy. Example \u00b6 os : environment : http_proxy : http://myserver https_proxy : http://myserver install.mode \u00b6 Definition \u00b6 Harvester installation mode: create : Creating a new Harvester installation. join : Join an existing Harvester installation. Need to specify server_url . Example \u00b6 install : mode : create install.networks \u00b6 Definition \u00b6 Configure network interfaces for the host machine. Each key-value pair represents a network interface. The key name becomes the network name, and the values are configurations for each network. Valid configuration fields are: method : Method to assign an IP to this network. The following are supported: static : Manually assign an IP and gateway. dhcp : Request an IP from the DHCP server. none : Do nothing. Useful when the interface does not need an IP, such as when creating VLAN network NIC in Harvester. ip : Static IP for this network. Required if static method is chosen. subnet_mask : Subnet mask for this network. Required if static method is chosen. gateway : Gateway for this network. Required if static method is chosen. interfaces : An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name : The name of the slave interface for the bonded network. default_route : Set the network as the default route or not. bond_options : Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlb miimon: 100 Note A network called harvester-mgmt is mandatory to establish a valid management network . Note Harvester uses the systemd net naming scheme . Please make sure the interface name is present on the target machine before installation. Example \u00b6 install : mode : create networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 default_route : true method : dhcp bond_options : mode : balance-tlb miimon : 100 harvester-vlan : # The VLAN network bond name. User can then input `harvester-vlan` in the VLAN NIC setting in the GUI. interfaces : - name : ens6 method : none bond_options : mode : balance-tlb miimon : 100 bond0 : interfaces : - name : ens8 method : static ip : 10.10.18.2 subnet_mask : 255.255.255.0 gateway : 192.168.11.1 install.force_efi \u00b6 Force EFI installation even when EFI is not detected. Default: false . install.device \u00b6 The device to install the OS. install.silent \u00b6 Reserved. install.iso_url \u00b6 ISO to download and install from if booting from kernel/vmlinuz and not ISO. install.poweroff \u00b6 Shutdown the machine after installation instead of rebooting install.no_format \u00b6 Do not partition and format, assume layout exists already. install.debug \u00b6 Run the installation with additional logging and debugging enabled for the installed system. install.tty \u00b6 Definition \u00b6 The tty device used for the console. Example \u00b6 install : tty : ttyS0,115200n8 install.vip \u00b6 install.vip_mode \u00b6 install.vip_hw_addr \u00b6 Definition \u00b6 install.vip : The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://<VIP> . install.vip_mode dhcp : Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided. static : Harvester uses a static VIP. install.vip_hw_addr : The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp . Example \u00b6 Configure a static VIP. install : vip : 192.168.0.100 vip_mode : static Configure a DHCP VIP. install : vip : 10.10.0.19 vip_mode : dhcp vip_hw_addr : 52:54:00:ec:0e:0b","title":"Harvester Configuration"},{"location":"install/harvester-configuration/#harvester-configuration","text":"","title":"Harvester Configuration"},{"location":"install/harvester-configuration/#configuration-example","text":"Harvester configuration file can be provided during manual or automatic installation to configure various settings. The following is a configuration example: server_url : https://someserver:8443 token : TOKEN_VALUE os : ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB... - github:username write_files : - encoding : \"\" content : test content owner : root path : /etc/test.txt permissions : '0755' hostname : myhost modules : - kvm - nvme sysctls : kernel.printk : \"4 4 1 7\" kernel.kptr_restrict : \"1\" dns_nameservers : - 8.8.8.8 - 1.1.1.1 ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org password : rancher environment : http_proxy : http://myserver https_proxy : http://myserver install : mode : create networks : harvester-mgmt : interfaces : - name : ens5 default_route : true method : dhcp force_efi : true device : /dev/vda silent : true iso_url : http://myserver/test.iso poweroff : true no_format : true debug : true tty : ttyS0 vip : 10.10.0.19 vip_hw_addr : 52:54:00:ec:0e:0b vip_mode : dhcp","title":"Configuration Example"},{"location":"install/harvester-configuration/#configuration-reference","text":"Below is a reference of all configuration keys. Warning Security Risks : The configuration file contains credentials which should be kept secret. Please do not make the configuration file publicly accessible.","title":"Configuration Reference"},{"location":"install/harvester-configuration/#server_url","text":"","title":"server_url"},{"location":"install/harvester-configuration/#definition","text":"The URL of the Harvester server to join as an agent. This configuration is mandatory when the installation is in JOIN mode. It tells the Harvester installer where the main server is.","title":"Definition"},{"location":"install/harvester-configuration/#example","text":"server_url : https://someserver:8443 install : mode : join","title":"Example"},{"location":"install/harvester-configuration/#token","text":"","title":"token"},{"location":"install/harvester-configuration/#definition_1","text":"The cluster secret or node token. If the value matches the format of a node token it will automatically be assumed to be a node token. Otherwise it is treated as a cluster secret. In order for a new node to join the Harvester cluster, the token should match what the server has.","title":"Definition"},{"location":"install/harvester-configuration/#example_1","text":"token : myclustersecret Or a node token token : \"K1074ec55daebdf54ef48294b0ddf0ce1c3cb64ee7e3d0b9ec79fbc7baf1f7ddac6::node:77689533d0140c7019416603a05275d4\"","title":"Example"},{"location":"install/harvester-configuration/#osssh_authorized_keys","text":"","title":"os.ssh_authorized_keys"},{"location":"install/harvester-configuration/#definition_2","text":"A list of SSH authorized keys that should be added to the default user, rancher . SSH keys can be obtained from GitHub user accounts by using the format github:${USERNAME} . This is done by downloading the keys from https://github.com/${USERNAME}.keys .","title":"Definition"},{"location":"install/harvester-configuration/#example_2","text":"os : ssh_authorized_keys : - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2TBZGjE+J8ag11dzkFT58J3XPONrDVmalCNrKxsfADfyy0eqdZrG8hcAxAR/5zuj90Gin2uBR4Sw6Cn4VHsPZcFpXyQCjK1QDADj+WcuhpXOIOY3AB0LZBly9NI0ll+8lo3QtEaoyRLtrMBhQ6Mooy2M3MTG4JNwU9o3yInuqZWf9PvtW6KxMl+ygg1xZkljhemGZ9k0wSrjqif+8usNbzVlCOVQmZwZA+BZxbdcLNwkg7zWJSXzDIXyqM6iWPGXQDEbWLq3+HR1qKucTCSxjbqoe0FD5xcW7NHIME5XKX84yH92n6yn+rxSsyUfhJWYqJd+i0fKf5UbN6qLrtd/D\" - \"github:ibuildthecloud\"","title":"Example"},{"location":"install/harvester-configuration/#oswrite_files","text":"A list of files to write to disk on boot. The encoding field specifies the content's encoding. Valid encoding values are: \"\" : content data are written in plain text. In this case, the encoding field can be also omitted. b64 , base64 : content data are base64-encoded. gz , gzip : content data are gzip-compressed. gz+base64 , gzip+base64 , gz+b64 , gzip+b64 : content data are gzip-compressed first and then base64-encoded. Example os : write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4... owner : root:root path : /etc/connman/main.conf permissions : '0644' - content : | # My new /etc/sysconfig/samba file SMDBOPTIONS=\"-D\" path : /etc/sysconfig/samba - content : !!binary | f0VMRgIBAQAAAAAAAAAAAAIAPgABAAAAwARAAAAAAABAAAAAAAAAAJAVAAAAAA AEAAHgAdAAYAAAAFAAAAQAAAAAAAAABAAEAAAAAAAEAAQAAAAAAAwAEAAAAAAA AAAAAAAAAwAAAAQAAAAAAgAAAAAAAAACQAAAAAAAAAJAAAAAAAAcAAAAAAAAAB ... path : /bin/arch permissions : '0555' - content : | 15 * * * * root ship_logs path : /etc/crontab","title":"os.write_files"},{"location":"install/harvester-configuration/#oshostname","text":"","title":"os.hostname"},{"location":"install/harvester-configuration/#definition_3","text":"Set the system hostname. If the system hostname is supplied via DHCP, then that value will be used here. If this value is not set and one is not supplied via DHCP, then a random hostname will be generated.","title":"Definition"},{"location":"install/harvester-configuration/#example_3","text":"os : hostname : myhostname","title":"Example"},{"location":"install/harvester-configuration/#osmodules","text":"","title":"os.modules"},{"location":"install/harvester-configuration/#definition_4","text":"A list of kernel modules to be loaded on start.","title":"Definition"},{"location":"install/harvester-configuration/#example_4","text":"os : modules : - kvm - nvme","title":"Example"},{"location":"install/harvester-configuration/#ossysctls","text":"","title":"os.sysctls"},{"location":"install/harvester-configuration/#definition_5","text":"Kernel sysctl to set up on start. These are the typical configurations found in /etc/sysctl.conf . Values must be specified as strings.","title":"Definition"},{"location":"install/harvester-configuration/#example_5","text":"os : sysctls : kernel.printk : 4 4 1 7 # the YAML parser will read as a string kernel.kptr_restrict : \"1\" # force the YAML parser to read as a string","title":"Example"},{"location":"install/harvester-configuration/#osdns_nameservers","text":"","title":"os.dns_nameservers"},{"location":"install/harvester-configuration/#definition_6","text":"Fallback DNS name servers to use if DNS is not configured by DHCP or in the OS.","title":"Definition"},{"location":"install/harvester-configuration/#example_6","text":"os : dns_nameservers : - 8.8.8.8 - 1.1.1.1","title":"Example"},{"location":"install/harvester-configuration/#osntp_servers","text":"","title":"os.ntp_servers"},{"location":"install/harvester-configuration/#definition_7","text":"Fallback ntp servers to use if NTP is not configured elsewhere in the OS.","title":"Definition"},{"location":"install/harvester-configuration/#example_7","text":"os : ntp_servers : - 0.us.pool.ntp.org - 1.us.pool.ntp.org","title":"Example"},{"location":"install/harvester-configuration/#ospassword","text":"","title":"os.password"},{"location":"install/harvester-configuration/#definition_8","text":"The password for the default user, rancher . By default, there is no password for the rancher user. If you set a password at runtime it will be reset on the next boot. The value of the password can be clear text or an encrypted form. The easiest way to get this encrypted form is to change your password on a Linux system and copy the value of the second field from /etc/shadow . You can also encrypt a password using openssl passwd -1 .","title":"Definition"},{"location":"install/harvester-configuration/#example_8","text":"Encrypted: os : password : \"$1$tYtghCfK$QHa51MS6MVAcfUKuOzNKt0\" Or clear text: os : password : supersecure","title":"Example"},{"location":"install/harvester-configuration/#osenvironment","text":"","title":"os.environment"},{"location":"install/harvester-configuration/#definition_9","text":"Environment variables to be set on K3s and other processes like the boot process. Primary use of this field is to set the HTTP proxy.","title":"Definition"},{"location":"install/harvester-configuration/#example_9","text":"os : environment : http_proxy : http://myserver https_proxy : http://myserver","title":"Example"},{"location":"install/harvester-configuration/#installmode","text":"","title":"install.mode"},{"location":"install/harvester-configuration/#definition_10","text":"Harvester installation mode: create : Creating a new Harvester installation. join : Join an existing Harvester installation. Need to specify server_url .","title":"Definition"},{"location":"install/harvester-configuration/#example_10","text":"install : mode : create","title":"Example"},{"location":"install/harvester-configuration/#installnetworks","text":"","title":"install.networks"},{"location":"install/harvester-configuration/#definition_11","text":"Configure network interfaces for the host machine. Each key-value pair represents a network interface. The key name becomes the network name, and the values are configurations for each network. Valid configuration fields are: method : Method to assign an IP to this network. The following are supported: static : Manually assign an IP and gateway. dhcp : Request an IP from the DHCP server. none : Do nothing. Useful when the interface does not need an IP, such as when creating VLAN network NIC in Harvester. ip : Static IP for this network. Required if static method is chosen. subnet_mask : Subnet mask for this network. Required if static method is chosen. gateway : Gateway for this network. Required if static method is chosen. interfaces : An array of interface names. If provided, the installer then combines these NICs into a single logical bonded interface. interfaces.name : The name of the slave interface for the bonded network. default_route : Set the network as the default route or not. bond_options : Options for bonded interfaces. Refer to here for more info. If not provided, the following options would be used: mode: balance-tlb miimon: 100 Note A network called harvester-mgmt is mandatory to establish a valid management network . Note Harvester uses the systemd net naming scheme . Please make sure the interface name is present on the target machine before installation.","title":"Definition"},{"location":"install/harvester-configuration/#example_11","text":"install : mode : create networks : harvester-mgmt : # The management bond name. This is mandatory. interfaces : - name : ens5 default_route : true method : dhcp bond_options : mode : balance-tlb miimon : 100 harvester-vlan : # The VLAN network bond name. User can then input `harvester-vlan` in the VLAN NIC setting in the GUI. interfaces : - name : ens6 method : none bond_options : mode : balance-tlb miimon : 100 bond0 : interfaces : - name : ens8 method : static ip : 10.10.18.2 subnet_mask : 255.255.255.0 gateway : 192.168.11.1","title":"Example"},{"location":"install/harvester-configuration/#installforce_efi","text":"Force EFI installation even when EFI is not detected. Default: false .","title":"install.force_efi"},{"location":"install/harvester-configuration/#installdevice","text":"The device to install the OS.","title":"install.device"},{"location":"install/harvester-configuration/#installsilent","text":"Reserved.","title":"install.silent"},{"location":"install/harvester-configuration/#installiso_url","text":"ISO to download and install from if booting from kernel/vmlinuz and not ISO.","title":"install.iso_url"},{"location":"install/harvester-configuration/#installpoweroff","text":"Shutdown the machine after installation instead of rebooting","title":"install.poweroff"},{"location":"install/harvester-configuration/#installno_format","text":"Do not partition and format, assume layout exists already.","title":"install.no_format"},{"location":"install/harvester-configuration/#installdebug","text":"Run the installation with additional logging and debugging enabled for the installed system.","title":"install.debug"},{"location":"install/harvester-configuration/#installtty","text":"","title":"install.tty"},{"location":"install/harvester-configuration/#definition_12","text":"The tty device used for the console.","title":"Definition"},{"location":"install/harvester-configuration/#example_12","text":"install : tty : ttyS0,115200n8","title":"Example"},{"location":"install/harvester-configuration/#installvip","text":"","title":"install.vip"},{"location":"install/harvester-configuration/#installvip_mode","text":"","title":"install.vip_mode"},{"location":"install/harvester-configuration/#installvip_hw_addr","text":"","title":"install.vip_hw_addr"},{"location":"install/harvester-configuration/#definition_13","text":"install.vip : The VIP of the Harvester management endpoint. After installation, users can access the Harvester GUI at the URL https://<VIP> . install.vip_mode dhcp : Harvester will send DHCP requests to get the VIP. The install.vip_hw_addr field needs to be provided. static : Harvester uses a static VIP. install.vip_hw_addr : The hardware address corresponding to the VIP. Users must configure their on-premise DHCP server to offer the configured VIP. The field is mandatory when install.vip_mode is dhcp .","title":"Definition"},{"location":"install/harvester-configuration/#example_13","text":"Configure a static VIP. install : vip : 192.168.0.100 vip_mode : static Configure a DHCP VIP. install : vip : 10.10.0.19 vip_mode : dhcp vip_hw_addr : 52:54:00:ec:0e:0b","title":"Example"},{"location":"install/iso-install/","text":"ISO Installation \u00b6 To get the Harvester ISO, download it from the Github releases. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer option. Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device that Harvester will be formatted to. Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt , and the IP address can either be configured via DHCP or a static assigned a static one. (Optional) Configure the DNS servers. Use commas as a delimiter. Configure the Virtual IP which you can use to access the cluster or join the cluster to other nodes. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher . (Optional) Configure the NTP Servers of the node. This defaults to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with a cloud-init config, enter the HTTP URL here. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, it will restart the host. After the restart, the Harvester console containing the management URL and status will be displayed. You can Use F12 to switch between the Harvester console and the Shell. The default URL of the web interface is https://your-virtual-ip . You will be prompted to set the password for the default admin user when logging in for the first time.","title":"ISO Installation"},{"location":"install/iso-install/#iso-installation","text":"To get the Harvester ISO, download it from the Github releases. During the installation you can either choose to form a new cluster, or join the node to an existing cluster. Note: This video shows a quick overview of the ISO installation. Mount the Harvester ISO disk and boot the server by selecting the Harvester Installer option. Choose the installation mode by either creating a new Harvester cluster, or by joining an existing one. Choose the installation device that Harvester will be formatted to. Configure the hostname and select the network interface for the management network. By default, Harvester will create a bonded NIC named harvester-mgmt , and the IP address can either be configured via DHCP or a static assigned a static one. (Optional) Configure the DNS servers. Use commas as a delimiter. Configure the Virtual IP which you can use to access the cluster or join the cluster to other nodes. Configure the cluster token . This token will be used for adding other nodes to the cluster. Configure the login password of the host. The default SSH user is rancher . (Optional) Configure the NTP Servers of the node. This defaults to 0.suse.pool.ntp.org . (Optional) If you need to use an HTTP proxy to access the outside world, enter the proxy URL address here. Otherwise, leave this blank. (Optional) You can choose to import SSH keys from a remote URL server. Your GitHub public keys can be used with https://github.com/<username>.keys . (Optional) If you need to customize the host with a cloud-init config, enter the HTTP URL here. After confirming the installation options, Harvester will be installed to your host. The installation may take a few minutes to be complete. Once the installation is complete, it will restart the host. After the restart, the Harvester console containing the management URL and status will be displayed. You can Use F12 to switch between the Harvester console and the Shell. The default URL of the web interface is https://your-virtual-ip . You will be prompted to set the password for the default admin user when logging in for the first time.","title":"ISO Installation"},{"location":"install/management-address/","text":"Management Address \u00b6 Harvester provides a fixed virtual IP (VIP) as the management address. Users can see the management address on the console dashboard after installation. Usages \u00b6 The management address has two usages. Allow users to access the Harvester UI via https protocol. Used by the other nodes to join the cluster. Configure VIP \u00b6 Users can specify the VIP during installation. It can either be configured via DHCP or assigned a static one. Note: In PXE boot, Harvester does not support setting the VIP via DHCP. It will be addressed in the next release. Issue: https://github.com/harvester/harvester/issues/1410","title":"Management Address"},{"location":"install/management-address/#management-address","text":"Harvester provides a fixed virtual IP (VIP) as the management address. Users can see the management address on the console dashboard after installation.","title":"Management Address"},{"location":"install/management-address/#usages","text":"The management address has two usages. Allow users to access the Harvester UI via https protocol. Used by the other nodes to join the cluster.","title":"Usages"},{"location":"install/management-address/#configure-vip","text":"Users can specify the VIP during installation. It can either be configured via DHCP or assigned a static one. Note: In PXE boot, Harvester does not support setting the VIP via DHCP. It will be addressed in the next release. Issue: https://github.com/harvester/harvester/issues/1410","title":"Configure VIP"},{"location":"install/pxe-boot-install/","text":"PXE Boot Installation \u00b6 Starting from version 0.2.0 , Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples . Prerequisite \u00b6 Important Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs. Preparing HTTP Servers \u00b6 An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10 , and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/ . Preparing Boot Files \u00b6 Download the required files from the Harvester releases page . The ISO: harvester-<version>-amd64.iso The kernel: harvester-<version>-vmlinuz-amd64 The initrd: harvester-<version>-initrd-amd64 The rootfs squashfs image: harvester-<version>-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/ Preparing iPXE Boot Scripts \u00b6 When performing an automatic installation, there are two modes: CREATE : we are installing a node to construct an initial Harvester cluster. JOIN : we are installing a node to join an existing Harvester cluster. CREATE Mode \u00b6 Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token : token os : hostname : node1 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password : rancher install : mode : create networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 default_route : true method : dhcp bond_options : mode : balance-tlb miimon : 100 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso vip : 10.100.0.100 # The VIP to access the Harvester GUI. Make sure the IP is free to use. vip_mode : static # Or dhcp, check configuration file for more information. For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create . Note If there are multiple network interfaces on the installing machine, the user can use ip=<interface>:dhcp to specify the booting interface (e.g., ip=eth1:dhcp ). JOIN Mode \u00b6 Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url : https://10.100.0.130:8443 token : token os : hostname : node2 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers : - 1.1.1.1 - 8.8.8.8 password : rancher install : mode : join networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 default_route : true method : dhcp bond_options : mode : balance-tlb miimon : 10 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join . DHCP Server Configuration \u00b6 The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16 ; subnet 10 .100.0.0 netmask 255 .255.255.0 { option routers 10 .100.0.10 ; option domain-name-servers 192 .168.2.1 ; range 10 .100.0.100 10 .100.0.253 ; } group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } group { # join group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-join-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-join\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node2 { hardware ethernet 52 :54:00:69:d5:92 ; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first. Harvester Configuration \u00b6 For more information about Harvester configuration, please refer to the Harvester configuration page. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file. UEFI HTTP Boot support \u00b6 UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation. Serve the iPXE Program \u00b6 Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally. DHCP Server Configuration \u00b6 If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } elsif substring ( option vendor-class-identifier, 0 , 10 ) = \"HTTPClient\" { # UEFI HTTP Boot option vendor-class-identifier \"HTTPClient\" ; filename \"http://10.100.0.10/harvester/ipxe.efi\" ; } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi . The iPXE Script for UEFI Boot \u00b6 It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-<version>-vmlinuz initrd=harvester-<version>-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot The parameter initrd=harvester-<version>-initrd is required.","title":"PXE Boot Installation"},{"location":"install/pxe-boot-install/#pxe-boot-installation","text":"Starting from version 0.2.0 , Harvester can be installed automatically. This document provides an example to do an automatic installation with PXE boot. We recommend using iPXE to perform the network boot. It has more features than the traditional PXE Boot program and is likely available in modern NIC cards. If the iPXE firmware is not available for your NIC card, the iPXE firmware images can be loaded from the TFTP server first. To see sample iPXE scripts, please visit Harvester iPXE Examples .","title":"PXE Boot Installation"},{"location":"install/pxe-boot-install/#prerequisite","text":"Important Nodes need to have at least 8G of RAM because the installer loads the full ISO file into tmpfs.","title":"Prerequisite"},{"location":"install/pxe-boot-install/#preparing-http-servers","text":"An HTTP server is required to serve boot files. Let's assume the NGINX HTTP server's IP is 10.100.0.10 , and it serves the /usr/share/nginx/html/ directory with the path http://10.100.0.10/ .","title":"Preparing HTTP Servers"},{"location":"install/pxe-boot-install/#preparing-boot-files","text":"Download the required files from the Harvester releases page . The ISO: harvester-<version>-amd64.iso The kernel: harvester-<version>-vmlinuz-amd64 The initrd: harvester-<version>-initrd-amd64 The rootfs squashfs image: harvester-<version>-rootfs-amd64.squashfs Serve the files. Copy or move the downloaded files to an appropriate location so they can be downloaded via the HTTP server. For example: sudo mkdir -p /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-amd64.iso /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-vmlinuz-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-initrd-amd64 /usr/share/nginx/html/harvester/ sudo cp /path/to/harvester-<version>-rootfs-amd64.squashfs /usr/share/nginx/html/harvester/","title":"Preparing Boot Files"},{"location":"install/pxe-boot-install/#preparing-ipxe-boot-scripts","text":"When performing an automatic installation, there are two modes: CREATE : we are installing a node to construct an initial Harvester cluster. JOIN : we are installing a node to join an existing Harvester cluster.","title":"Preparing iPXE Boot Scripts"},{"location":"install/pxe-boot-install/#create-mode","text":"Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-create.yaml for CREATE mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-create.yaml token : token os : hostname : node1 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin password : rancher install : mode : create networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 default_route : true method : dhcp bond_options : mode : balance-tlb miimon : 100 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso vip : 10.100.0.100 # The VIP to access the Harvester GUI. Make sure the IP is free to use. vip_mode : static # Or dhcp, check configuration file for more information. For machines that needs to be installed using CREATE mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-create . Note If there are multiple network interfaces on the installing machine, the user can use ip=<interface>:dhcp to specify the booting interface (e.g., ip=eth1:dhcp ).","title":"CREATE Mode"},{"location":"install/pxe-boot-install/#join-mode","text":"Warning Security Risks : The configuration file below contains credentials which should be kept secret. Please do not make the configuration file publicly accessible. Create a Harvester configuration file called config-join.yaml for JOIN mode. Modify the values as needed: # cat /usr/share/nginx/html/harvester/config-join.yaml server_url : https://10.100.0.130:8443 token : token os : hostname : node2 ssh_authorized_keys : - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDbeUa9A7Kee+hcCleIXYxuaPksn2m4PZTd4T7wPcse8KbsQfttGRax6vxQXoPO6ehddqOb2nV7tkW2mEhR50OE7W7ngDHbzK2OneAyONYF44bmMsapNAGvnsBKe9rNrev1iVBwOjtmyVLhnLrJIX+2+3T3yauxdu+pmBsnD5OIKUrBrN1sdwW0rA2rHDiSnzXHNQM3m02aY6mlagdQ/Ovh96h05QFCHYxBc6oE/mIeFRaNifa4GU/oELn3a6HfbETeBQz+XOEN+IrLpnZO9riGyzsZroB/Y3Ju+cJxH06U0B7xwJCRmWZjuvfFQUP7RIJD1gRGZzmf3h8+F+oidkO2i5rbT57NaYSqkdVvR6RidVLWEzURZIGbtHjSPCi4kqD05ua8r/7CC0PvxQb1O5ILEdyJr2ZmzhF6VjjgmyrmSmt/yRq8MQtGQxyKXZhJqlPYho4d5SrHi5iGT2PvgDQaWch0I3ndEicaaPDZJHWBxVsCVAe44Wtj9g3LzXkyu3k= root@admin dns_nameservers : - 1.1.1.1 - 8.8.8.8 password : rancher install : mode : join networks : harvester-mgmt : # (Mandatory) The management bond name. interfaces : - name : ens5 default_route : true method : dhcp bond_options : mode : balance-tlb miimon : 10 harvester-vlan : # (Optional) The VLAN network bond name. If VLAN NIC names vary from interfaces : # host to host, consider creating a bonding device. Users can then select - name : ens6 # `harvester-vlan` as the VLAN network NIC in the Harvester GUI. method : none bond_options : mode : balance-tlb miimon : 100 device : /dev/sda iso_url : http://10.100.0.10/harvester/harvester-<version>-amd64.iso Note that the mode is join and the server_url needs to be provided. For machines that needs to be installed in JOIN mode, the following is an iPXE script that boots the kernel with the above config: #!ipxe kernel harvester-<version>-vmlinuz ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-join.yaml initrd harvester-<version>-initrd boot This assumes the iPXE script is stored in /usr/share/nginx/html/harvester/ipxe-join .","title":"JOIN Mode"},{"location":"install/pxe-boot-install/#dhcp-server-configuration","text":"The following is an example of how to configure the ISC DHCP server to offer iPXE scripts: option architecture-type code 93 = unsigned integer 16 ; subnet 10 .100.0.0 netmask 255 .255.255.0 { option routers 10 .100.0.10 ; option domain-name-servers 192 .168.2.1 ; range 10 .100.0.100 10 .100.0.253 ; } group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } group { # join group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-join-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-join\" ; } } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node2 { hardware ethernet 52 :54:00:69:d5:92 ; } } The config file declares a subnet and two groups. The first group is for hosts to boot using CREATE mode and the other one is for JOIN mode. By default, the iPXE path is chosen, but if it sees a PXE client it offers the iPXE image according to the client architecture. Please prepare those images and a TFTP server first.","title":"DHCP Server Configuration"},{"location":"install/pxe-boot-install/#harvester-configuration","text":"For more information about Harvester configuration, please refer to the Harvester configuration page. Users can also provide configuration via kernel parameters. For example, to specify the CREATE install mode, users can pass the harvester.install.mode=create kernel parameter when booting. Values passed through kernel parameters have higher priority than values specified in the config file.","title":"Harvester Configuration"},{"location":"install/pxe-boot-install/#uefi-http-boot-support","text":"UEFI firmware supports loading a boot image from an HTTP server. This section demonstrates how to use UEFI HTTP boot to load the iPXE program and perform an automatic installation.","title":"UEFI HTTP Boot support"},{"location":"install/pxe-boot-install/#serve-the-ipxe-program","text":"Download the iPXE UEFI program from http://boot.ipxe.org/ipxe.efi and make sure ipxe.efi can be downloaded from the HTTP server. For example: cd /usr/share/nginx/html/harvester/ wget http://boot.ipxe.org/ipxe.efi The file now can be downloaded from http://10.100.0.10/harvester/ipxe.efi locally.","title":"Serve the iPXE Program"},{"location":"install/pxe-boot-install/#dhcp-server-configuration_1","text":"If the user plans to use the UEFI HTTP boot feature by getting a dynamic IP first, the DHCP server needs to provide the iPXE program URL when it sees such a request. The following is an updated ISC DHCP server group example: group { # create group if exists user-class and option user-class = \"iPXE\" { # iPXE Boot if option architecture-type = 00 :07 { filename \"http://10.100.0.10/harvester/ipxe-create-efi\" ; } else { filename \"http://10.100.0.10/harvester/ipxe-create\" ; } } elsif substring ( option vendor-class-identifier, 0 , 10 ) = \"HTTPClient\" { # UEFI HTTP Boot option vendor-class-identifier \"HTTPClient\" ; filename \"http://10.100.0.10/harvester/ipxe.efi\" ; } else { # PXE Boot if option architecture-type = 00 :07 { # UEFI filename \"ipxe.efi\" ; } else { # Non-UEFI filename \"undionly.kpxe\" ; } } host node1 { hardware ethernet 52 :54:00:6b:13:e2 ; } } The elsif substring statement is new, and it offers http://10.100.0.10/harvester/ipxe.efi when it sees a UEFI HTTP boot DHCP request. After the client fetches the iPXE program and runs it, the iPXE program will send a DHCP request again and load the iPXE script from the URL http://10.100.0.10/harvester/ipxe-create-efi .","title":"DHCP Server Configuration"},{"location":"install/pxe-boot-install/#the-ipxe-script-for-uefi-boot","text":"It's mandatory to specify the initrd image for UEFI boot in the kernel parameters. The following is an updated version of iPXE script for CREATE mode. #!ipxe kernel harvester-<version>-vmlinuz initrd=harvester-<version>-initrd ip=dhcp net.ifnames=1 rd.cos.disable rd.noverifyssl console=tty1 root=live:http://10.100.0.10/harvester/rootfs.squashfs harvester.install.automatic=true harvester.install.config_url=http://10.100.0.10/harvester/config-create.yaml initrd harvester-<version>-initrd boot The parameter initrd=harvester-<version>-initrd is required.","title":"The iPXE Script for UEFI Boot"},{"location":"install/usb-install/","text":"USB Installation \u00b6 Create a bootable USB flash drive \u00b6 There are a couple of ways to create a USB installation flash drive. balenaEtcher \u00b6 balenaEtcher supports writing images to USB flash drives. It has a GUI and is easy to use. Select the Harvester installation ISO and the target USB device to create a USB installation flash drive. dd command \u00b6 On Linux or other platforms that have the dd command, users can use dd to create a USB installation flash drive. Warning Make sure you choose the correct device. The process erases data on the selected device. # sudo dd if=<path_to_iso> of=<path_to_usb_device> bs=64k Common issues \u00b6 When booting from a USB installation flash drive, a GRUB _ text is displayed, but nothing happens \u00b6 If you are using the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. e.g., Select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. Note the representation varies from system to system. Graphics issue \u00b6 Firmwares of some graphic cards are not shipped in v0.3.0 . You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot. Other issues \u00b6 Harvester installer is not displayed If a USB flash driver boots, but you can't see the harvester installer. You may try out the following workarounds: Plug the USB flash drive into a USB 2.0 slot. For version v0.3.0 , try to remove the console=ttyS0 parameter when booting. You can press e to edit the GRUB menu entry and remove the console=ttyS0 parameter.","title":"USB Installation"},{"location":"install/usb-install/#usb-installation","text":"","title":"USB Installation"},{"location":"install/usb-install/#create-a-bootable-usb-flash-drive","text":"There are a couple of ways to create a USB installation flash drive.","title":"Create a bootable USB flash drive"},{"location":"install/usb-install/#balenaetcher","text":"balenaEtcher supports writing images to USB flash drives. It has a GUI and is easy to use. Select the Harvester installation ISO and the target USB device to create a USB installation flash drive.","title":"balenaEtcher"},{"location":"install/usb-install/#dd-command","text":"On Linux or other platforms that have the dd command, users can use dd to create a USB installation flash drive. Warning Make sure you choose the correct device. The process erases data on the selected device. # sudo dd if=<path_to_iso> of=<path_to_usb_device> bs=64k","title":"dd command"},{"location":"install/usb-install/#common-issues","text":"","title":"Common issues"},{"location":"install/usb-install/#when-booting-from-a-usb-installation-flash-drive-a-grub-_-text-is-displayed-but-nothing-happens","text":"If you are using the UEFI mode, try to boot from the UEFI boot partition on the USB device rather than the USB device itself. e.g., Select the UEFI: USB disk 3.0 PMAP, Partition 1 to boot. Note the representation varies from system to system.","title":"When booting from a USB installation flash drive, a GRUB _ text is displayed, but nothing happens"},{"location":"install/usb-install/#graphics-issue","text":"Firmwares of some graphic cards are not shipped in v0.3.0 . You can press e to edit the GRUB menu entry and append nomodeset to the boot parameters. Press Ctrl + x to boot.","title":"Graphics issue"},{"location":"install/usb-install/#other-issues","text":"Harvester installer is not displayed If a USB flash driver boots, but you can't see the harvester installer. You may try out the following workarounds: Plug the USB flash drive into a USB 2.0 slot. For version v0.3.0 , try to remove the console=ttyS0 parameter when booting. You can press e to edit the GRUB menu entry and remove the console=ttyS0 parameter.","title":"Other issues"},{"location":"monitoring/monitoring/","text":"Monitoring \u00b6 Available as of v0.3.0 Dashboard Metrics \u00b6 Harvester v0.3.0 has provided a built-in monitoring integration using Prometheus . Monitoring is automatically installed during ISO installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboard on the Grafana UI. Note Only admin users are able to view the dashboard metrics. VM Detail Metrics \u00b6 For each VM, users can view the VM metrics by clicking the VM details page.","title":"Monitoring"},{"location":"monitoring/monitoring/#monitoring","text":"Available as of v0.3.0","title":"Monitoring"},{"location":"monitoring/monitoring/#dashboard-metrics","text":"Harvester v0.3.0 has provided a built-in monitoring integration using Prometheus . Monitoring is automatically installed during ISO installations. From the Dashboard page, users can view the cluster metrics and top 10 most used VM metrics respectively. Also, users can click the Grafana dashboard link to view more dashboard on the Grafana UI. Note Only admin users are able to view the dashboard metrics.","title":"Dashboard Metrics"},{"location":"monitoring/monitoring/#vm-detail-metrics","text":"For each VM, users can view the VM metrics by clicking the VM details page.","title":"VM Detail Metrics"},{"location":"networking/harvester-network/","text":"Harvester Network \u00b6 Harvester is built on top of Kubernetes and leverages its built-in CNI mechanism to provide the interface between network providers and its VM networks. We have implemented the Harvester VLAN network based on the bridge CNI to provide a pure L2-mode network, that would bridge your VMs to the host network interface and can be connected using the physical switch for both internal and external network communication. Moreover, the Harvester UI integrates the harvester-network-controller to provide user-friendly VLAN network configurations, e.g., to create and manage VLAN networks or to add a VLAN network to the VM. Currently, Harvester supports two types of networks: Management network VLAN Management Network \u00b6 Harvester uses flannel CNI as its default management network. It is a built-in network that can be used directly from the cluster. However, the management network IP is not persisted and will be changed after a VM reboot. Additionally, users can leverage the Kubernetes service object to create a stable IP for your VMs with the management network. VLAN Network \u00b6 The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. The below diagram illustrates how the VLAN network works in Harvester. The Harvester network-controller creates a bridge for each node and a pair of veth for each VM to implement its VLAN network. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between VMs and the switch. VMs within the same VLAN can communicate with each other, while the VMs from different VLANs can't. The external switch ports connected to the hosts or other devices (such as the DHCP server) should be set as trunk or hybrid type and permit the specified VLANs. Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic. Enabling Default VLAN Network \u00b6 Users can enable the VLAN network via Setting > VLAN and selecting a common physical NIC for the nodes as the default VLAN config . It is recommended to choose a separate NIC for the VLAN other than the one used for the management network (i.e., harvester-mgmt ) for better network performance and isolation. Note Modifying the default VLAN network setting will not update the existing configured host network. Harvester VLAN network supports bond interfaces, currently it can only be created automatically via PEX Boot Configuration . Users may also login to the node and create it manually. Optional: Users can customize each node's VLAN network via the HOST > Network tab. Create a VLAN Network \u00b6 A new VLAN network can be created via the Advanced > Networks page and clicking the Create button. Specify the name and VLAN ID that you want to create for the VLAN network. Create a VM with VLAN Network \u00b6 Users can now create a new VM using the above configured VLAN network, Click the Create button on the Virtual Machines page. Specify the required parameters and click the Networks tab. Either configure the default network to be a VLAN network or select an additional network to add. Note Only the first NIC will be enabled by default. Users can either choose to use a management network or a VLAN network. You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. Users can choose to add one or multiple network interface cards. Additional network interface card can be enabled by default via setting the cloud-init network data. E.g., version : 1 config : - type : physical name : enp1s0 # name is varies upon OS image subnets : - type : dhcp - type : physical name : enp2s0 subnets : - type : DHCP For more detailed configs you may refer to the cloud-init network configs . Configure DHCP servers on Networks \u00b6 By default, the Harvester VLAN network would expect your router to provide a DHCP server that VMs can request and assign IP addresses automatically. If you are running Harvester in a virtual environment that does not contain a DHCP server, you may consider deploying a DHCP server manually in a node or using a containerized method. Refer to this issue as an example.","title":"Harvester Network"},{"location":"networking/harvester-network/#harvester-network","text":"Harvester is built on top of Kubernetes and leverages its built-in CNI mechanism to provide the interface between network providers and its VM networks. We have implemented the Harvester VLAN network based on the bridge CNI to provide a pure L2-mode network, that would bridge your VMs to the host network interface and can be connected using the physical switch for both internal and external network communication. Moreover, the Harvester UI integrates the harvester-network-controller to provide user-friendly VLAN network configurations, e.g., to create and manage VLAN networks or to add a VLAN network to the VM. Currently, Harvester supports two types of networks: Management network VLAN","title":"Harvester Network"},{"location":"networking/harvester-network/#management-network","text":"Harvester uses flannel CNI as its default management network. It is a built-in network that can be used directly from the cluster. However, the management network IP is not persisted and will be changed after a VM reboot. Additionally, users can leverage the Kubernetes service object to create a stable IP for your VMs with the management network.","title":"Management Network"},{"location":"networking/harvester-network/#vlan-network","text":"The Harvester network-controller leverages the multus and bridge CNI plugins to implement its customized L2 bridge VLAN network. It helps to connect your VMs to the host network interface and can be accessed from internal and external networks using the physical switch. The below diagram illustrates how the VLAN network works in Harvester. The Harvester network-controller creates a bridge for each node and a pair of veth for each VM to implement its VLAN network. The bridge acts as a switch to forward the network traffic from or to VMs and the veth pair is like the connected ports between VMs and the switch. VMs within the same VLAN can communicate with each other, while the VMs from different VLANs can't. The external switch ports connected to the hosts or other devices (such as the DHCP server) should be set as trunk or hybrid type and permit the specified VLANs. Users can use VLAN with PVID (default 1) to communicate with any normal untagged traffic.","title":"VLAN Network"},{"location":"networking/harvester-network/#enabling-default-vlan-network","text":"Users can enable the VLAN network via Setting > VLAN and selecting a common physical NIC for the nodes as the default VLAN config . It is recommended to choose a separate NIC for the VLAN other than the one used for the management network (i.e., harvester-mgmt ) for better network performance and isolation. Note Modifying the default VLAN network setting will not update the existing configured host network. Harvester VLAN network supports bond interfaces, currently it can only be created automatically via PEX Boot Configuration . Users may also login to the node and create it manually. Optional: Users can customize each node's VLAN network via the HOST > Network tab.","title":"Enabling Default VLAN Network"},{"location":"networking/harvester-network/#create-a-vlan-network","text":"A new VLAN network can be created via the Advanced > Networks page and clicking the Create button. Specify the name and VLAN ID that you want to create for the VLAN network.","title":"Create a VLAN Network"},{"location":"networking/harvester-network/#create-a-vm-with-vlan-network","text":"Users can now create a new VM using the above configured VLAN network, Click the Create button on the Virtual Machines page. Specify the required parameters and click the Networks tab. Either configure the default network to be a VLAN network or select an additional network to add. Note Only the first NIC will be enabled by default. Users can either choose to use a management network or a VLAN network. You will need to select the Install guest agent option in the Advanced Options tab to get the VLAN network IP address from the Harvester UI. Users can choose to add one or multiple network interface cards. Additional network interface card can be enabled by default via setting the cloud-init network data. E.g., version : 1 config : - type : physical name : enp1s0 # name is varies upon OS image subnets : - type : dhcp - type : physical name : enp2s0 subnets : - type : DHCP For more detailed configs you may refer to the cloud-init network configs .","title":"Create a VM with VLAN Network"},{"location":"networking/harvester-network/#configure-dhcp-servers-on-networks","text":"By default, the Harvester VLAN network would expect your router to provide a DHCP server that VMs can request and assign IP addresses automatically. If you are running Harvester in a virtual environment that does not contain a DHCP server, you may consider deploying a DHCP server manually in a node or using a containerized method. Refer to this issue as an example.","title":"Configure DHCP servers on Networks"},{"location":"rancher/cloud-provider/","text":"Harvester Cloud Provider \u00b6 Available as of v0.3.0 Users can now provision both RKE1 and RKE2 clusters in Rancher 2.6.1, using the built-in Harvester Node Driver. Harvester can now provide load balancer support as well as cluster Persistent Storage support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2. How to configure a LoadBalancer service . Deploying \u00b6 Prerequisites \u00b6 The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Deploying to the RKE1 Cluster with Harvester Node Driver \u00b6 When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select the External cloud provider. Generate add-on configuration and add it to the RKE YAML file. # depend on kubectl to operate the Harvester cluster curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | sh -s <serviceAccount name> <namespace> Deploying to the RKE2 Cluster with Harvester Node Driver \u00b6 When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically. Load Balancer Support \u00b6 After deploying the Harvester cloud provider, users can now configure a Kubernetes service of the type LoadBalancer . Currently, users can only set the load balancer configuration using service annotations . Example You can specify the Harvester LoadBalancer service config through annotations. The cloudprovider.harvesterhci.io/healthcheck-port annotation is required. For example: cloudprovider.harvesterhci.io/ipam: dhcp - if the network of your Kubernetes nodes supports DHCP. cloudprovider.harvesterhci.io/healthcheck-port: 80 - specify the port of your service. IPAM \u00b6 Harvester's built-in load balancer supports both pool and dhcp modes. Users can specify the IPAM mode using the annotation key cloudprovider.harvesterhci.io/ipam . This value defaults to pool . pool: You should configure an IP address pool in Harvester in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address pool for the load balancer. Refer to the guideline to configure an IP address pool. E.g, for a Namespace pool, a service will take an address based upon its namespace pool cidr/range-namespace. These would look like the following: $ kubectl get configmap -n kube-system kubevip -o yaml apiVersion : v1 kind : ConfigMap metadata : name : kubevip namespace : kube-system data : cidr-default : 192.168.0.200/29 cidr-development : 192.168.0.210/29 cidr-finance : 192.168.0.220/29 cidr-testing : 192.168.0.230/29 dhcp: A DHCP server is required. The Harvester LoadBalancer controller will request an IP address from the DHCP server of the Kubernetes nodes. Health Checks \u00b6 The Harvester load balancer supports TCP health checks. Supported annotations are shown below: Key Value Required Description cloudprovider.harvesterhci.io/healthcheck-port string true Specifies the port. The prober will access the address composed of the backend server IP and the port. cloudprovider.harvesterhci.io/healthcheck-success-threshold string false Specifies the health check success threshold. The default value is 1. If the number of times the prober continuously detects an address successfully reaches the success threshold, then the backend server can start to forward traffic. cloudprovider.harvesterhci.io/healthcheck-failure-threshold string false Specifies the success and failure threshold. The default value is 3. The backend server will stop forwarding traffic if the number of health check failures reaches the failure threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds string false Specifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds string false Specifies the timeout of every health check. The default value is 3 seconds.","title":"Harvester Cloud Provider"},{"location":"rancher/cloud-provider/#harvester-cloud-provider","text":"Available as of v0.3.0 Users can now provision both RKE1 and RKE2 clusters in Rancher 2.6.1, using the built-in Harvester Node Driver. Harvester can now provide load balancer support as well as cluster Persistent Storage support to the guest Kubernetes cluster. In this page we will learn: How to deploy the Harvester cloud provider in both RKE1 and RKE2. How to configure a LoadBalancer service .","title":"Harvester Cloud Provider"},{"location":"rancher/cloud-provider/#deploying","text":"","title":"Deploying"},{"location":"rancher/cloud-provider/#prerequisites","text":"The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace.","title":"Prerequisites"},{"location":"rancher/cloud-provider/#deploying-to-the-rke1-cluster-with-harvester-node-driver","text":"When spinning up an RKE cluster using the Harvester node driver, you can perform two steps to deploy the Harvester cloud provider: Select the External cloud provider. Generate add-on configuration and add it to the RKE YAML file. # depend on kubectl to operate the Harvester cluster curl -sfL https://raw.githubusercontent.com/harvester/cloud-provider-harvester/master/deploy/generate_addon.sh | sh -s <serviceAccount name> <namespace>","title":"Deploying to the RKE1 Cluster with Harvester Node Driver"},{"location":"rancher/cloud-provider/#deploying-to-the-rke2-cluster-with-harvester-node-driver","text":"When spinning up an RKE2 cluster using the Harvester node driver, select the Harvester cloud provider. The node driver will then help deploy both the CSI driver and CCM automatically.","title":"Deploying to the RKE2 Cluster with Harvester Node Driver"},{"location":"rancher/cloud-provider/#load-balancer-support","text":"After deploying the Harvester cloud provider, users can now configure a Kubernetes service of the type LoadBalancer . Currently, users can only set the load balancer configuration using service annotations . Example You can specify the Harvester LoadBalancer service config through annotations. The cloudprovider.harvesterhci.io/healthcheck-port annotation is required. For example: cloudprovider.harvesterhci.io/ipam: dhcp - if the network of your Kubernetes nodes supports DHCP. cloudprovider.harvesterhci.io/healthcheck-port: 80 - specify the port of your service.","title":"Load Balancer Support"},{"location":"rancher/cloud-provider/#ipam","text":"Harvester's built-in load balancer supports both pool and dhcp modes. Users can specify the IPAM mode using the annotation key cloudprovider.harvesterhci.io/ipam . This value defaults to pool . pool: You should configure an IP address pool in Harvester in advance. The Harvester LoadBalancer controller will allocate an IP address from the IP address pool for the load balancer. Refer to the guideline to configure an IP address pool. E.g, for a Namespace pool, a service will take an address based upon its namespace pool cidr/range-namespace. These would look like the following: $ kubectl get configmap -n kube-system kubevip -o yaml apiVersion : v1 kind : ConfigMap metadata : name : kubevip namespace : kube-system data : cidr-default : 192.168.0.200/29 cidr-development : 192.168.0.210/29 cidr-finance : 192.168.0.220/29 cidr-testing : 192.168.0.230/29 dhcp: A DHCP server is required. The Harvester LoadBalancer controller will request an IP address from the DHCP server of the Kubernetes nodes.","title":"IPAM"},{"location":"rancher/cloud-provider/#health-checks","text":"The Harvester load balancer supports TCP health checks. Supported annotations are shown below: Key Value Required Description cloudprovider.harvesterhci.io/healthcheck-port string true Specifies the port. The prober will access the address composed of the backend server IP and the port. cloudprovider.harvesterhci.io/healthcheck-success-threshold string false Specifies the health check success threshold. The default value is 1. If the number of times the prober continuously detects an address successfully reaches the success threshold, then the backend server can start to forward traffic. cloudprovider.harvesterhci.io/healthcheck-failure-threshold string false Specifies the success and failure threshold. The default value is 3. The backend server will stop forwarding traffic if the number of health check failures reaches the failure threshold. cloudprovider.harvesterhci.io/healthcheck-periodseconds string false Specifies the health check period. The default value is 5 seconds. cloudprovider.harvesterhci.io/healthcheck-timeoutseconds string false Specifies the timeout of every health check. The default value is 3 seconds.","title":"Health Checks"},{"location":"rancher/csi-driver/","text":"Harvester CSI Driver \u00b6 The Harvester Container Storage Interface (CSI) Driver provides a CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance. Deploying \u00b6 Prerequisites \u00b6 The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace. Deploying with Harvester RKE2 Node Driver \u00b6 When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when Harvester cloud provider is selected. Deploying with Harvester RKE1 Node Driver \u00b6 Select the external cloud provider option. Generate addon configuration and add it in the RKE config YAML. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh <serviceaccount name> <namespace>","title":"Harvester CSI Driver"},{"location":"rancher/csi-driver/#harvester-csi-driver","text":"The Harvester Container Storage Interface (CSI) Driver provides a CSI interface used by guest Kubernetes clusters in Harvester. It connects to the host cluster and hot-plugs host volumes to the virtual machines (VMs) to provide native storage performance.","title":"Harvester CSI Driver"},{"location":"rancher/csi-driver/#deploying","text":"","title":"Deploying"},{"location":"rancher/csi-driver/#prerequisites","text":"The Kubernetes cluster is built on top of Harvester virtual machines. The Harvester virtual machines run as guest Kubernetes nodes are in the same namespace.","title":"Prerequisites"},{"location":"rancher/csi-driver/#deploying-with-harvester-rke2-node-driver","text":"When spinning up a Kubernetes cluster using Rancher RKE2 node driver, the Harvester CSI driver will be deployed when Harvester cloud provider is selected.","title":"Deploying with Harvester RKE2 Node Driver"},{"location":"rancher/csi-driver/#deploying-with-harvester-rke1-node-driver","text":"Select the external cloud provider option. Generate addon configuration and add it in the RKE config YAML. # depend on kubectl to operate the Harvester cluster ./deploy/generate_addon.sh <serviceaccount name> <namespace>","title":"Deploying with Harvester RKE1 Node Driver"},{"location":"rancher/node-driver/","text":"Harvester Node Driver \u00b6 The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver , and the project repo is available at harvester/docker-machine-driver-harvester . Users can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.1 or above using the built-in Harvester node driver. Additionally, Harvester now can provide built-in Load Balancer support as well as raw cluster persistent storage support to the guest Kubernetes cluster. Note Currently only Rancher v2.6.1 or above is compatible with Harvester v0.3.0. Enable Harvester Node Driver \u00b6 The Harvester node driver is not enabled by default from the Rancher UI. Click the Cluster Management tab to enable the Harvester node driver. Click the Drivers page, then click the Node Drivers tab Select the Harvester node driver, then click Activate to enable the Harvester node driver Now users can spin up Kubernetes clusters on top of the Harvester cluster and manage them there. RKE1 Kubernetes Cluster \u00b6 Click to learn how to create RKE1 Kubernetes Clusters . RKE2 Kubernetes Cluster \u00b6 Click to learn how to create RKE2 Kubernetes Clusters .","title":"Harvester Node Driver"},{"location":"rancher/node-driver/#harvester-node-driver","text":"The Harvester node driver is used to provision VMs in the Harvester cluster. In this section, you'll learn how to configure Rancher to use the Harvester node driver to launch and manage Kubernetes clusters. A node driver is the same as a Docker Machine driver , and the project repo is available at harvester/docker-machine-driver-harvester . Users can now provision RKE1/RKE2 Kubernetes clusters in Rancher v2.6.1 or above using the built-in Harvester node driver. Additionally, Harvester now can provide built-in Load Balancer support as well as raw cluster persistent storage support to the guest Kubernetes cluster. Note Currently only Rancher v2.6.1 or above is compatible with Harvester v0.3.0.","title":"Harvester Node Driver"},{"location":"rancher/node-driver/#enable-harvester-node-driver","text":"The Harvester node driver is not enabled by default from the Rancher UI. Click the Cluster Management tab to enable the Harvester node driver. Click the Drivers page, then click the Node Drivers tab Select the Harvester node driver, then click Activate to enable the Harvester node driver Now users can spin up Kubernetes clusters on top of the Harvester cluster and manage them there.","title":"Enable Harvester Node Driver"},{"location":"rancher/node-driver/#rke1-kubernetes-cluster","text":"Click to learn how to create RKE1 Kubernetes Clusters .","title":"RKE1 Kubernetes Cluster"},{"location":"rancher/node-driver/#rke2-kubernetes-cluster","text":"Click to learn how to create RKE2 Kubernetes Clusters .","title":"RKE2 Kubernetes Cluster"},{"location":"rancher/rancher-integration/","text":"Rancher Integration \u00b6 Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Harvester has integrated Rancher by default starting with Rancher v2.6.1. Users can now import and manage multiple Harvester clusters using the Rancher Virtualization Management page and leverage the Rancher authentication feature and RBAC control for multi-tenancy support. Deploying Rancher \u00b6 Previously in Harvester v0.2.0, users had the option to enable the embedded Rancher server. This option has been removed from Harvester v0.3.0 . To use Rancher with Harvester, please install the Rancher server separately from the Harvester. As an option, You can spin up a VM in the Harvester and install the Rancher v2.6.1 or above to try out the integration features. Quick Start Guide \u00b6 Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM) An on-premises VM A bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection. From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:v2.6.2 Note For more information about how to deploy the Rancher server, please refer to the Rancher documentation Virtualization Management \u00b6 With Rancher's Virtualization Management, users can now import and manage Harvester clusters. By clicking on one of the clusters, users are able to view and manage the downstream Harvester resources such as VMs, images, volumes, etc. Additionally, Rancher's VM feature has leveraged existing Rancher features such as authentication with various auth providers and multi-tenant support. For more details, please reference the virtualization management page. Note Virtualization Management is in Tech Preview. Creating Kubernetes Clusters using the Harvester Node Driver \u00b6 Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage Kubernetes clusters. Starting with Rancher v2.6.1 , the Harvester node driver has been added by default. Users can reference this doc for more details. Note Harvester Node Driver is in Tech Preview.","title":"Rancher integration"},{"location":"rancher/rancher-integration/#rancher-integration","text":"Available as of v0.3.0 Rancher is an open-source multi-cluster management platform. Harvester has integrated Rancher by default starting with Rancher v2.6.1. Users can now import and manage multiple Harvester clusters using the Rancher Virtualization Management page and leverage the Rancher authentication feature and RBAC control for multi-tenancy support.","title":"Rancher Integration"},{"location":"rancher/rancher-integration/#deploying-rancher","text":"Previously in Harvester v0.2.0, users had the option to enable the embedded Rancher server. This option has been removed from Harvester v0.3.0 . To use Rancher with Harvester, please install the Rancher server separately from the Harvester. As an option, You can spin up a VM in the Harvester and install the Rancher v2.6.1 or above to try out the integration features.","title":"Deploying Rancher"},{"location":"rancher/rancher-integration/#quick-start-guide","text":"Begin creation of a custom cluster by provisioning a Linux host. Your host can be any of the following: A cloud-hosted virtual machine (VM) An on-premises VM A bare-metal server Log into your Linux host using your preferred shell, such as PuTTy or a remote terminal connection. From your shell, enter the following command: # for a quick evaluation, you can run the Rancher server with the following command $ sudo docker run -d --restart = unless-stopped -p 80 :80 -p 443 :443 --privileged rancher/rancher:v2.6.2 Note For more information about how to deploy the Rancher server, please refer to the Rancher documentation","title":"Quick Start Guide"},{"location":"rancher/rancher-integration/#virtualization-management","text":"With Rancher's Virtualization Management, users can now import and manage Harvester clusters. By clicking on one of the clusters, users are able to view and manage the downstream Harvester resources such as VMs, images, volumes, etc. Additionally, Rancher's VM feature has leveraged existing Rancher features such as authentication with various auth providers and multi-tenant support. For more details, please reference the virtualization management page. Note Virtualization Management is in Tech Preview.","title":"Virtualization Management"},{"location":"rancher/rancher-integration/#creating-kubernetes-clusters-using-the-harvester-node-driver","text":"Harvester node driver is used to provision VMs in the Harvester cluster, which Rancher uses to launch and manage Kubernetes clusters. Starting with Rancher v2.6.1 , the Harvester node driver has been added by default. Users can reference this doc for more details. Note Harvester Node Driver is in Tech Preview.","title":"Creating Kubernetes Clusters using the Harvester Node Driver"},{"location":"rancher/rke1-cluster/","text":"Creating an RKE1 Kubernetes Cluster \u00b6 Users can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. Note Harvester RKE1 node driver is in tech preview. VLAN network is required for Harvester node driver. Create Your Cloud Credentials \u00b6 Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name. Select \"Imported Harvester\" or \"External Harvester\". Click Create . Create Node Template \u00b6 You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials . Configure Instance Options : Configure the CPU, memory, and disk Select an OS image that is compatible with the cloud-init config. Select a network that the node driver is able to connect to; currently, only VLAN is supported. Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu . Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information. Create RKE1 Kubernetes Cluster \u00b6 Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE1 . Select Harvester node driver. Enter Cluster Name (required). Enter Name Prefix (required). Enter Template (required). Select etcd and Control Plane (required). Click Create .","title":"Creating an RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#creating-an-rke1-kubernetes-cluster","text":"Users can now provision RKE1 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. Note Harvester RKE1 node driver is in tech preview. VLAN network is required for Harvester node driver.","title":"Creating an RKE1 Kubernetes Cluster"},{"location":"rancher/rke1-cluster/#create-your-cloud-credentials","text":"Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name. Select \"Imported Harvester\" or \"External Harvester\". Click Create .","title":"Create Your Cloud Credentials"},{"location":"rancher/rke1-cluster/#create-node-template","text":"You can use the Harvester node driver to create node templates and eventually node pools for your Kubernetes cluster. Configure the Cloud Credentials . Configure Instance Options : Configure the CPU, memory, and disk Select an OS image that is compatible with the cloud-init config. Select a network that the node driver is able to connect to; currently, only VLAN is supported. Enter the SSH User; the username will be used to ssh to nodes. For example, a default user of the Ubuntu cloud image will be ubuntu . Enter a RANCHER TEMPLATE name. See nodes hosted by an infrastructure provider for more information.","title":"Create Node Template"},{"location":"rancher/rke1-cluster/#create-rke1-kubernetes-cluster","text":"Users can create an RKE1 Kubernetes cluster from the Cluster Management page via the Harvester RKE1 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE1 . Select Harvester node driver. Enter Cluster Name (required). Enter Name Prefix (required). Enter Template (required). Select etcd and Control Plane (required). Click Create .","title":"Create RKE1 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/","text":"Creating an RKE2 Kubernetes Cluster \u00b6 Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. Note Harvester RKE2 node driver is in tech preview. VLAN network is required for Harvester node driver. Create Your Cloud Credentials \u00b6 Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\". Click Create . Create RKE2 Kubernetes Cluster \u00b6 Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE2/K3s . Select Harvester node driver. Select a Cloud Credential . Enter Cluster Name (required). Enter Namespace (required). Enter Image (required). Enter Network Name (required). Enter SSH User (required). Click Create . Note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration. Currently only imported Harvester clusters are supported automatically.","title":"Creating an RKE2 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/#creating-an-rke2-kubernetes-cluster","text":"Users can now provision RKE2 Kubernetes clusters on top of the Harvester cluster in Rancher v2.6.1+ using the built-in Harvester node driver. Note Harvester RKE2 node driver is in tech preview. VLAN network is required for Harvester node driver.","title":"Creating an RKE2 Kubernetes Cluster"},{"location":"rancher/rke2-cluster/#create-your-cloud-credentials","text":"Click \u2630 > Cluster Management . Click Cloud Credentials . Click Create . Click Harvester . Enter your cloud credential name Select \"Imported Harvester\" or \"External Harvester\". Click Create .","title":"Create Your Cloud Credentials"},{"location":"rancher/rke2-cluster/#create-rke2-kubernetes-cluster","text":"Users can create a RKE2 Kubernetes cluster from the Cluster Management page via the RKE2 node driver. Select Clusters menu. Click Create button. Toggle Switch to RKE2/K3s . Select Harvester node driver. Select a Cloud Credential . Enter Cluster Name (required). Enter Namespace (required). Enter Image (required). Enter Network Name (required). Enter SSH User (required). Click Create . Note RKE2 v1.21.5+rke2r2 or above provides a built-in Harvester Cloud Provider and Guest CSI driver integration. Currently only imported Harvester clusters are supported automatically.","title":"Create RKE2 Kubernetes Cluster"},{"location":"rancher/virtualization-management/","text":"Virtualization Management \u00b6 For Harvester v0.3.0 and above, virtualization management with the multi-cluster management feature will be supported using Rancher v2.6.x. First, you will need to install Rancher v2.6.1 or above. For testing purposes, you can spin up a Rancher server using the following docker run command: $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.2 Note For a production environment setup, please refer to the official Rancher docs . Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server. Specify the Cluster Name and click Create . You will then see the registration commands; copy the appropriate command and ssh to one of the Harvester management nodes to run this command accordingly. Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly. From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page. Multi-Tenancy \u00b6 In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication , users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions : Define user authorization outside the scope of any particular cluster. Cluster and Project Roles : Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC . Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings. A project user can be assigned to a specific project with permission to manage the resources inside the project. Multi-Tenancy Example \u00b6 The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users & Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project. A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab. Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save . Open an incognito browser and log in as project-owner . After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster to which you have been assigned. Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed. Create a VM with one of the images that you have uploaded. Log in with another user, e.g., project-readonly , and this user will only have the read permission of this project. Note A known issue was found that allows the read-only user to be able to manage API actions . Delete Imported Harvester Cluster \u00b6 Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management > Harvester Clusters . Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. Warning Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","title":"Virtualization management"},{"location":"rancher/virtualization-management/#virtualization-management","text":"For Harvester v0.3.0 and above, virtualization management with the multi-cluster management feature will be supported using Rancher v2.6.x. First, you will need to install Rancher v2.6.1 or above. For testing purposes, you can spin up a Rancher server using the following docker run command: $ sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher:v2.6.2 Note For a production environment setup, please refer to the official Rancher docs . Once the Rancher server is up and running, log in and click the hamburger menu and choose the Virtualization Management tab. Select Import Existing to import the downstream Harvester cluster into the Rancher server. Specify the Cluster Name and click Create . You will then see the registration commands; copy the appropriate command and ssh to one of the Harvester management nodes to run this command accordingly. Once the agent node is ready, you should be able to view and access the imported Harvester cluster from the Rancher server and manage your VMs accordingly. From the Harvester UI, you can click the hamburger menu to navigate back to the Rancher multi-cluster management page.","title":"Virtualization Management"},{"location":"rancher/virtualization-management/#multi-tenancy","text":"In Harvester, we have leveraged the existing Rancher RBAC authorization such that users can view and manage a set of resources based on their cluster and project role permissions. Within Rancher, each person authenticates as a user, which is a login that grants a user access to Rancher. As mentioned in Authentication , users can either be local or external. Once the user logs into Rancher, their authorization, also known as access rights, is determined by global permissions and cluster and project roles. Global Permissions : Define user authorization outside the scope of any particular cluster. Cluster and Project Roles : Define user authorization inside the specific cluster or project where users are assigned the role. Both global permissions and cluster and project roles are implemented on top of Kubernetes RBAC . Therefore, enforcement of permissions and roles is performed by Kubernetes. A cluster owner has full control over the cluster and all resources inside it, e.g., hosts, VMs, volumes, images, networks, backups, and settings. A project user can be assigned to a specific project with permission to manage the resources inside the project.","title":"Multi-Tenancy"},{"location":"rancher/virtualization-management/#multi-tenancy-example","text":"The following example provides a good explanation of how the multi-tenant feature works: First, add new users via the Rancher Users & Authentication page. Then click Create to add two new separated users, such as project-owner and project-readonly respectively. A project-owner is a user with permission to manage a list of resources of a particular project, e.g., the default project. A project-readonly is a user with read-only permission of a particular project, e.g., the default project. Click one of the imported Harvester clusters after navigating to the Harvester UI. Click the Projects/Namespaces tab. Select a project such as default and click the Edit Config menu to assign the users to this project with appropriate permissions. For example, the project-owner user will be assigned the project owner role. Continue to add the project-readonly user to the same project with read-only permissions and click Save . Open an incognito browser and log in as project-owner . After logging in as the project-owner user, click the Virtualization Management tab. There you should be able to view the cluster to which you have been assigned. Click the Images tab to view a list of images previously uploaded to the harvester-public namespace. You can also upload your own image if needed. Create a VM with one of the images that you have uploaded. Log in with another user, e.g., project-readonly , and this user will only have the read permission of this project. Note A known issue was found that allows the read-only user to be able to manage API actions .","title":"Multi-Tenancy Example"},{"location":"rancher/virtualization-management/#delete-imported-harvester-cluster","text":"Users can delete the imported Harvester cluster from the Rancher UI via Virtualization Management > Harvester Clusters . Select the cluster you want to remove and click the Delete button to delete the imported Harvester cluster. Warning Please do not run the kubectl delete -f ... command to delete the imported Harvester cluster as it will remove the entire cattle-system namespace which is required of the Harvester cluster.","title":"Delete Imported Harvester Cluster"},{"location":"reference/api/","text":"API Reference \u00b6 const ui = SwaggerUIBundle({ url: 'swagger.json', dom_id: '#swagger-ui', })","title":"API Reference"},{"location":"reference/api/#api-reference","text":"const ui = SwaggerUIBundle({ url: 'swagger.json', dom_id: '#swagger-ui', })","title":"API Reference"},{"location":"terraform/terraform/","text":"Harvester Terraform Provider \u00b6 Requirements \u00b6 Terraform >= 0.13.x Go 1.16 to build the provider plugin Install the Provider \u00b6 Option 1: Download and install the provider from the Terraform registry . \u00b6 To install this provider, copy and paste this code into your Terraform configuration. Then, run terraform init. Terraform 0.13+ terraform { required_providers { harvester = { source = \"harvester/harvester\" version = \"0.2.8\" } } } provider \"harvester\" { # Configuration options } For more details, please refer to the Harvester provider documentation . Option 2: Build and install the provider manually. \u00b6 Building the provider: \u00b6 Clone the repository using the following command: git clone git@github.com:harvester/terraform-provider-harvester Enter the provider directory and build the provider; this will build the provider and put the provider binary in ./bin . Use the following command: cd terraform-provider-harvester make Installing the provider: \u00b6 The expected location for the Harvester provider for the target platform within one of the local search directories is as follows: registry.terraform.io/harvester/harvester/0.2.8/linux_amd64/terraform-provider-harvester_v0.2.8 The default location for locally-installed providers will be one of the following, depending on the operating system under which you are running Terraform: Windows: %APPDATA%\\terraform.d\\plugins All other systems: ~/.terraform.d/plugins Place the provider into the plugin directory as in the following example: version=0.2.8 arch=linux_amd64 terraform_harvester_provider_bin=./bin/terraform-provider-harvester terraform_harvester_provider_dir=\"${HOME}/.terraform.d/plugins/registry.terraform.io/harvester/harvester/${version}/${arch}/\" mkdir -p \"${terraform_harvester_provider_dir}\" cp ${terraform_harvester_provider_bin} \"${terraform_harvester_provider_dir}/terraform-provider-harvester_v${version}\"} Using the provider \u00b6 After placing the provider into your plugins directory, run terraform init to initialize it. More information about provider-specific configuration options can be found on the docs directory","title":"Harvester Terraform Provider"},{"location":"terraform/terraform/#harvester-terraform-provider","text":"","title":"Harvester Terraform Provider"},{"location":"terraform/terraform/#requirements","text":"Terraform >= 0.13.x Go 1.16 to build the provider plugin","title":"Requirements"},{"location":"terraform/terraform/#install-the-provider","text":"","title":"Install the Provider"},{"location":"terraform/terraform/#option-1-download-and-install-the-provider-from-the-terraform-registry","text":"To install this provider, copy and paste this code into your Terraform configuration. Then, run terraform init. Terraform 0.13+ terraform { required_providers { harvester = { source = \"harvester/harvester\" version = \"0.2.8\" } } } provider \"harvester\" { # Configuration options } For more details, please refer to the Harvester provider documentation .","title":"Option 1: Download and install the provider from the Terraform registry."},{"location":"terraform/terraform/#option-2-build-and-install-the-provider-manually","text":"","title":"Option 2: Build and install the provider manually."},{"location":"terraform/terraform/#building-the-provider","text":"Clone the repository using the following command: git clone git@github.com:harvester/terraform-provider-harvester Enter the provider directory and build the provider; this will build the provider and put the provider binary in ./bin . Use the following command: cd terraform-provider-harvester make","title":"Building the provider:"},{"location":"terraform/terraform/#installing-the-provider","text":"The expected location for the Harvester provider for the target platform within one of the local search directories is as follows: registry.terraform.io/harvester/harvester/0.2.8/linux_amd64/terraform-provider-harvester_v0.2.8 The default location for locally-installed providers will be one of the following, depending on the operating system under which you are running Terraform: Windows: %APPDATA%\\terraform.d\\plugins All other systems: ~/.terraform.d/plugins Place the provider into the plugin directory as in the following example: version=0.2.8 arch=linux_amd64 terraform_harvester_provider_bin=./bin/terraform-provider-harvester terraform_harvester_provider_dir=\"${HOME}/.terraform.d/plugins/registry.terraform.io/harvester/harvester/${version}/${arch}/\" mkdir -p \"${terraform_harvester_provider_dir}\" cp ${terraform_harvester_provider_bin} \"${terraform_harvester_provider_dir}/terraform-provider-harvester_v${version}\"}","title":"Installing the provider:"},{"location":"terraform/terraform/#using-the-provider","text":"After placing the provider into your plugins directory, run terraform init to initialize it. More information about provider-specific configuration options can be found on the docs directory","title":"Using the provider"},{"location":"troubleshooting/harvester/","text":"Generate a support bundle \u00b6 Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle. Access Embedded Rancher \u00b6 You can access the embedded Rancher dashboard via https://{{HARVESTER_IP}}/dashboard/c/local/explorer . Note We only support to use the embedded Rancher dashboard for debugging and validation purpose. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here . Access Embedded Longhorn \u00b6 You can access the embedded Longhorn UI via https://{{HARVESTER_IP}}/dashboard/c/local/longhorn . Note We only support to use the embedded Longhorn UI for debugging and validation purpose .","title":"Harvester"},{"location":"troubleshooting/harvester/#generate-a-support-bundle","text":"Users can generate a support bundle in the Harvester GUI with the following steps: Click the Support link at the bottom-left of Harvester Web UI. Click Generate Support Bundle button. Enter a useful description for the support bundle and click Create to generate and download a support bundle.","title":"Generate a support bundle"},{"location":"troubleshooting/harvester/#access-embedded-rancher","text":"You can access the embedded Rancher dashboard via https://{{HARVESTER_IP}}/dashboard/c/local/explorer . Note We only support to use the embedded Rancher dashboard for debugging and validation purpose. For Rancher's multi-cluster and multi-tenant integration, please refer to the docs here .","title":"Access Embedded Rancher"},{"location":"troubleshooting/harvester/#access-embedded-longhorn","text":"You can access the embedded Longhorn UI via https://{{HARVESTER_IP}}/dashboard/c/local/longhorn . Note We only support to use the embedded Longhorn UI for debugging and validation purpose .","title":"Access Embedded Longhorn"},{"location":"troubleshooting/installation/","text":"Installation \u00b6 The following sections contain tips to troubleshoot or get assistance with failed installations. Logging into the Harvester Installer (a live OS) \u00b6 Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancher Password: rancher Meeting hardware requirements \u00b6 Check that your hardware meets the minimum requirements to complete installation. Receiving the message \"Loading images. This may take a few minutes...\" \u00b6 Because the system doesn't have a default route, your installer may become \"stuck\" in this state. You can check your route status by executing the following command: $ ip route default via 10.10.0.10 dev harvester-mgmt proto dhcp <-- Does a default route exist? 10.10.0.0/24 dev harvester-mgmt proto kernel scope link src 10.10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too. Collecting information \u00b6 Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. Content of these files: /var/log/console.log /run/cos/target/rke2.log /tmp/harvester.* /tmp/cos.* Output of these commands: blkid dmesg","title":"Installation"},{"location":"troubleshooting/installation/#installation","text":"The following sections contain tips to troubleshoot or get assistance with failed installations.","title":"Installation"},{"location":"troubleshooting/installation/#logging-into-the-harvester-installer-a-live-os","text":"Users can press the key combination CTRL + ALT + F2 to switch to another TTY and log in with the following credentials: User: rancher Password: rancher","title":"Logging into the Harvester Installer (a live OS)"},{"location":"troubleshooting/installation/#meeting-hardware-requirements","text":"Check that your hardware meets the minimum requirements to complete installation.","title":"Meeting hardware requirements"},{"location":"troubleshooting/installation/#receiving-the-message-loading-images-this-may-take-a-few-minutes","text":"Because the system doesn't have a default route, your installer may become \"stuck\" in this state. You can check your route status by executing the following command: $ ip route default via 10.10.0.10 dev harvester-mgmt proto dhcp <-- Does a default route exist? 10.10.0.0/24 dev harvester-mgmt proto kernel scope link src 10.10.0.15 Check that your DHCP server offers a default route option. Attaching content from /run/cos/target/rke2.log is helpful too.","title":"Receiving the message \"Loading images. This may take a few minutes...\""},{"location":"troubleshooting/installation/#collecting-information","text":"Please include the following information in a bug report when reporting a failed installation: A failed installation screenshot. Content of these files: /var/log/console.log /run/cos/target/rke2.log /tmp/harvester.* /tmp/cos.* Output of these commands: blkid dmesg","title":"Collecting information"},{"location":"troubleshooting/os/","text":"Operating System \u00b6 Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the cOS toolkit . The following sections contain information and tips to help users troubleshoot OS-related issues. How to log into a Harvester node \u00b6 Users can log into a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~> sudo blkid # Or become root rancher@node1:~> sudo -i node1:~ # blkid How can I install packages? Why are some paths read-only? \u00b6 The OS file system, like a container image, is image-based and immutable except in some directories. To temporarily enable the read-write mode, please use the following steps: Warning Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0 , we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat > /oem/91_hack.yaml <<'EOF' name: \"Rootfs Layout Settings for debugrw\" stages: rootfs: - if: 'grep -q root=LABEL=COS_ACTIVE /proc/cmdline && grep -q rd.cos.debugrw /proc/cmdline' name: \"Layout configuration for debugrw\" environment_file: /run/cos/cos-layout.env environment: RW_PATHS: \" \" EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. How to permanently edit kernel parameters \u00b6 Note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry \"Harvester ea6e7f5-dirty\" --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd nomodeset initrd (loop0)$initramfs } Reboot for changes to take effect. How to change the default GRUB boot menu entry \u00b6 To change the default entry, first check the --id attribute of a menu entry, as in the following example: # cat /run/initramfs/cos-state/grub2/grub.cfg <...> menuentry \"Harvester ea6e7f5-dirty (debug)\" --id cos-debug { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img The id of the above entry is cos-debug . We can then set the default entry by: # grub2-editenv /oem/grubenv set saved_entry=cos-debug How to debug a system crash or hang \u00b6 Collect crash log \u00b6 If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. Note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs. Collect crash dumps \u00b6 For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/<time> directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","title":"Operating System"},{"location":"troubleshooting/os/#operating-system","text":"Harvester runs on an OpenSUSE-based OS. The OS is an artifact produced by the cOS toolkit . The following sections contain information and tips to help users troubleshoot OS-related issues.","title":"Operating System"},{"location":"troubleshooting/os/#how-to-log-into-a-harvester-node","text":"Users can log into a Harvester node with the username rancher and the password or SSH keypair provided during installation. The user rancher can execute privileged commands without entering a password: # Run a privileged command rancher@node1:~> sudo blkid # Or become root rancher@node1:~> sudo -i node1:~ # blkid","title":"How to log into a Harvester node"},{"location":"troubleshooting/os/#how-can-i-install-packages-why-are-some-paths-read-only","text":"The OS file system, like a container image, is image-based and immutable except in some directories. To temporarily enable the read-write mode, please use the following steps: Warning Enabling read-write mode might break your system if files are modified. Please use it at your own risk. For version v0.3.0 , we need to apply a workaround first to make some directories non-overlaid after enabling read-write mode. On a running Harvester node, run the following command as root: cat > /oem/91_hack.yaml <<'EOF' name: \"Rootfs Layout Settings for debugrw\" stages: rootfs: - if: 'grep -q root=LABEL=COS_ACTIVE /proc/cmdline && grep -q rd.cos.debugrw /proc/cmdline' name: \"Layout configuration for debugrw\" environment_file: /run/cos/cos-layout.env environment: RW_PATHS: \" \" EOF Reboot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append rd.cos.debugrw to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system.","title":"How can I install packages? Why are some paths read-only?"},{"location":"troubleshooting/os/#how-to-permanently-edit-kernel-parameters","text":"Note The following steps are a workaround. Harvester will inform the community once a permanent resolution is in place. Re-mount state directory in rw mode: # blkid -L COS_STATE /dev/vda2 # mount -o remount,rw /dev/vda2 /run/initramfs/cos-state Edit the grub config file and append parameters to the linux (loop0)$kernel $kernelcmd line. The following example adds a nomodeset parameter: # vim /run/initramfs/cos-state/grub2/grub.cfg menuentry \"Harvester ea6e7f5-dirty\" --id cos { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img set root=($root) source (loop0)/etc/cos/bootargs.cfg linux (loop0)$kernel $kernelcmd nomodeset initrd (loop0)$initramfs } Reboot for changes to take effect.","title":"How to permanently edit kernel parameters"},{"location":"troubleshooting/os/#how-to-change-the-default-grub-boot-menu-entry","text":"To change the default entry, first check the --id attribute of a menu entry, as in the following example: # cat /run/initramfs/cos-state/grub2/grub.cfg <...> menuentry \"Harvester ea6e7f5-dirty (debug)\" --id cos-debug { search.fs_label COS_STATE root set img=/cOS/active.img set label=COS_ACTIVE loopback loop0 /$img The id of the above entry is cos-debug . We can then set the default entry by: # grub2-editenv /oem/grubenv set saved_entry=cos-debug","title":"How to change the default GRUB boot menu entry"},{"location":"troubleshooting/os/#how-to-debug-a-system-crash-or-hang","text":"","title":"How to debug a system crash or hang"},{"location":"troubleshooting/os/#collect-crash-log","text":"If kernel panic traces are not recorded in the system log when a system crashes, one reliable way to locate the crash log is to use a serial console. To enable outputting of kernel messages to a serial console, please use the following steps: Boot the system to GRUB menu. Press ESC to stay on the menu. Press e on first menuentry. Append console=ttyS0,115200n8 to the linux (loop0)$kernel $kernelcmd line. Press Ctrl + x to boot the system. Note Adjust the console options according to your environment. Make sure to append the console= string at the end of the line. Connect to the serial port to capture logs.","title":"Collect crash log"},{"location":"troubleshooting/os/#collect-crash-dumps","text":"For kernel panic crashes, you can use kdump to collect crash dumps. By default, the OS is booted without the kdump feature enabled. Users can enable the feature by selecting the debug menuentry when booting, as in the following example: When a system crashes, a crash dump will be stored in the /var/crash/<time> directory. Providing the crash dump to developers helps them to troubleshoot and resolve issues.","title":"Collect crash dumps"},{"location":"vm/access-to-the-vm/","text":"Access to the Virtual Machine (VM) \u00b6 Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client. Access with the Harvester UI \u00b6 VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, as with the Ubuntu minimal cloud image, the VM can be accessed with the serial console. Access with the SSH Client \u00b6 Enter the IP address of the host in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@<ip-address-or-hostname>","title":"Access to the Virtual Machine (VM)"},{"location":"vm/access-to-the-vm/#access-to-the-virtual-machine-vm","text":"Once the VM is up and running, you can access it using either the Virtual Network Computing (VNC) client or the serial console from the Harvester UI. Additionally, you can connect directly from your computer's SSH client.","title":"Access to the Virtual Machine (VM)"},{"location":"vm/access-to-the-vm/#access-with-the-harvester-ui","text":"VMs can be accessed from the UI directly using either VNC or the serial console. If the VGA display is not enabled on the VM, as with the Ubuntu minimal cloud image, the VM can be accessed with the serial console.","title":"Access with the Harvester UI"},{"location":"vm/access-to-the-vm/#access-with-the-ssh-client","text":"Enter the IP address of the host in a terminal emulation client, such as PuTTY. You may also run the following command to access the VM directly from your computer's SSH client: ssh -i ~/.ssh/your-ssh-key user@<ip-address-or-hostname>","title":"Access with the SSH Client"},{"location":"vm/backup-restore/","text":"VM Backup & Restore \u00b6 Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. Note A backup target must be set up. For more information, see Configure Backup Target . If the backup target has not been set, you\u2019ll be prompted with a message to do so. Configure Backup Target \u00b6 A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings > backup-target . Parameter Type Description Type string Choose S3 or NFS Endpoint string A hostname or an IP address. It can be left empty for AWS S3. BucketName string Name of the bucket BucketRegion string Region of the bucket AccessKeyID string A user-id that uniquely identifies your account SecretAccessKey string The password to your account Certificate string Paste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle bool Use VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS Create a VM backup \u00b6 Once the backup target is set, go to the Virtual Machines page. Click Take Backup of the VM actions to create a new VM backup. Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Advanced > Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM or replace an existing VM using this backup. Restore a new VM using a backup \u00b6 To restore a new VM from a backup, follow these steps: Go to the Backups page. Specify the new VM name and click Create . A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page. Replace an Existing VM using a backup \u00b6 You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page. Click Create . The restore process can be viewed from the Virtual Machines page. Restore a new VM on another Harvester cluster \u00b6 Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata & content backup feature. Prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover. Upload the same VM images to a new cluster \u00b6 Check the existing image name (normally starts with image- ) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat <<EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: \"\" pvcNamespace: \"\" sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF Restore a new VM in a new cluster \u00b6 Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster. Go to the Backups page. Select the synced VM backup metadata and choose to restore a new VM with a specified VM name. A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page.","title":"VM Backup & Restore"},{"location":"vm/backup-restore/#vm-backup-restore","text":"Available as of v0.3.0 VM backups are created from the Virtual Machines page. The VM backup volumes will be stored in the Backup Target (an NFS or S3 server), and they can be used to either restore a new VM or replace an existing VM. Note A backup target must be set up. For more information, see Configure Backup Target . If the backup target has not been set, you\u2019ll be prompted with a message to do so.","title":"VM Backup &amp; Restore"},{"location":"vm/backup-restore/#configure-backup-target","text":"A backup target is an endpoint used to access a backup store in Harvester. A backup store is an NFS server or S3 compatible server that stores the backups of VM volumes. The backup target can be set at Settings > backup-target . Parameter Type Description Type string Choose S3 or NFS Endpoint string A hostname or an IP address. It can be left empty for AWS S3. BucketName string Name of the bucket BucketRegion string Region of the bucket AccessKeyID string A user-id that uniquely identifies your account SecretAccessKey string The password to your account Certificate string Paste to use a self-signed SSL certificate of your S3 server VirtualHostedStyle bool Use VirtualHostedStyle access only; e.g., Alibaba Cloud (Aliyun) OSS","title":"Configure Backup Target"},{"location":"vm/backup-restore/#create-a-vm-backup","text":"Once the backup target is set, go to the Virtual Machines page. Click Take Backup of the VM actions to create a new VM backup. Set a custom backup name and click Create to create a new VM backup. Result: The backup is created. You will receive a notification message, and you can also go to the Advanced > Backups page to view all VM backups. The ReadyToUse status will be set to true once the Backup is complete. Users can either choose to restore a new VM or replace an existing VM using this backup.","title":"Create a VM backup"},{"location":"vm/backup-restore/#restore-a-new-vm-using-a-backup","text":"To restore a new VM from a backup, follow these steps: Go to the Backups page. Specify the new VM name and click Create . A new VM will be restored using the backup volumes and metadata, and you can access it from the Virtual Machines page.","title":"Restore a new VM using a backup"},{"location":"vm/backup-restore/#replace-an-existing-vm-using-a-backup","text":"You can replace an existing VM using the backup with the same VM backup target. You can choose to either delete or retain the previous volumes. By default, all previous volumes are deleted. Requirements: The VM must exist and is required to be in the powered-off status. Go to the Backups page. Click Create . The restore process can be viewed from the Virtual Machines page.","title":"Replace an Existing VM using a backup"},{"location":"vm/backup-restore/#restore-a-new-vm-on-another-harvester-cluster","text":"Available as of v1.0.0 Users can now restore a new VM on another cluster by leveraging the VM metadata & content backup feature. Prerequisites You must manually configure the virtual machine images with the same name on the new cluster first, otherwise the virtual machines will be failed to recover.","title":"Restore a new VM on another Harvester cluster"},{"location":"vm/backup-restore/#upload-the-same-vm-images-to-a-new-cluster","text":"Check the existing image name (normally starts with image- ) and create the same one on the new cluster. $ kubectl get vmimages -A NAMESPACE NAME DISPLAY-NAME SIZE AGE default image-79hdq focal-server-cloudimg-amd64.img 566886400 5h36m default image-l7924 harvester-v1.0.0-rc2-amd64.iso 3964551168 137m default image-lvqxn opensuse-leap-15.3.x86_64-nocloud.qcow2 568524800 5h35m Apply a VM image YAML with the same name and content in the new cluster. $ cat <<EOF | kubectl apply -f - apiVersion: harvesterhci.io/v1beta1 kind: VirtualMachineImage metadata: name: image-lvqxn namespace: default spec: displayName: opensuse-leap-15.3.x86_64-nocloud.qcow2 pvcName: \"\" pvcNamespace: \"\" sourceType: download url: http://download.opensuse.org/repositories/Cloud:/Images:/Leap_15.3/images/openSUSE-Leap-15.3.x86_64-NoCloud.qcow2 EOF","title":"Upload the same VM images to a new cluster"},{"location":"vm/backup-restore/#restore-a-new-vm-in-a-new-cluster","text":"Setup the same backup target in a new cluster. And the backup controller will automatically sync the backup metadata to the new cluster. Go to the Backups page. Select the synced VM backup metadata and choose to restore a new VM with a specified VM name. A new VM will be restored using the backup volumes and metadata. You can access it from the Virtual Machines page.","title":"Restore a new VM in a new cluster"},{"location":"vm/create-vm/","text":"Create a VM \u00b6 How to Create a VM \u00b6 Create one or more virtual machines from the Virtual Machines page. Choose the option to create either one or multiple VM instances. The VM Name is a required field. The VM Template is optional. You can select ISO, raw, and Windows image templates as default options. Configure the CPU and Memory of the VM. Select SSH keys or upload new keys. Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM. To configure networks, go to the Networks tab. The Management Network is added by default. It is also possible to add secondary networks to the VMs using VLAN networks. You may configure these on Advanced > Networks . Advanced options such as hostname and cloud-init data are optional. You may configure these in the Advanced Options section. Cloud Configuration Examples \u00b6 Password configuration for the default user: # cloud-config password : password chpasswd : { expire : False } ssh_pwauth : True Network-data configuration using DHCP: version : 1 config : - type : physical name : eth0 subnets : - type : dhcp - type : physical name : eth1 subnets : - type : dhcp You can also use the Cloud Config Template feature to include a pre-defined cloud-init configuration for the VM. Networks \u00b6 Management Network \u00b6 A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, a VM can be accessed via the management network. Secondary Network \u00b6 It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks .","title":"Create a VM"},{"location":"vm/create-vm/#create-a-vm","text":"","title":"Create a VM"},{"location":"vm/create-vm/#how-to-create-a-vm","text":"Create one or more virtual machines from the Virtual Machines page. Choose the option to create either one or multiple VM instances. The VM Name is a required field. The VM Template is optional. You can select ISO, raw, and Windows image templates as default options. Configure the CPU and Memory of the VM. Select SSH keys or upload new keys. Select a custom VM image on the Volumes tab. The default disk will be the root disk. You can add more disks to the VM. To configure networks, go to the Networks tab. The Management Network is added by default. It is also possible to add secondary networks to the VMs using VLAN networks. You may configure these on Advanced > Networks . Advanced options such as hostname and cloud-init data are optional. You may configure these in the Advanced Options section.","title":"How to Create a VM"},{"location":"vm/create-vm/#cloud-configuration-examples","text":"Password configuration for the default user: # cloud-config password : password chpasswd : { expire : False } ssh_pwauth : True Network-data configuration using DHCP: version : 1 config : - type : physical name : eth0 subnets : - type : dhcp - type : physical name : eth1 subnets : - type : dhcp You can also use the Cloud Config Template feature to include a pre-defined cloud-init configuration for the VM.","title":"Cloud Configuration Examples"},{"location":"vm/create-vm/#networks","text":"","title":"Networks"},{"location":"vm/create-vm/#management-network","text":"A management network represents the default VM eth0 interface configured by the cluster network solution that is present in each VM. By default, a VM can be accessed via the management network.","title":"Management Network"},{"location":"vm/create-vm/#secondary-network","text":"It is also possible to connect VMs using additional networks with Harvester's built-in VLAN networks .","title":"Secondary Network"},{"location":"vm/hotplug-volume/","text":"Hot-Plug Volumes \u00b6 Harvester supports adding hot-plug volumes to a running VM. Adding Hot-Plug Volumes to a Running VM \u00b6 The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page. Find the VM that you want to add a volume to and select \u22ee > Add Volume . Enter the Name and select the Volume . Click Apply .","title":"Hot-Plug Volumes"},{"location":"vm/hotplug-volume/#hot-plug-volumes","text":"Harvester supports adding hot-plug volumes to a running VM.","title":"Hot-Plug Volumes"},{"location":"vm/hotplug-volume/#adding-hot-plug-volumes-to-a-running-vm","text":"The following steps assume that you have a running VM and a ready volume: Go to the Virtual Machines page. Find the VM that you want to add a volume to and select \u22ee > Add Volume . Enter the Name and select the Volume . Click Apply .","title":"Adding Hot-Plug Volumes to a Running VM"},{"location":"vm/live-migration/","text":"Live Migration \u00b6 Live migration means moving a virtual machine to a different host without downtime. Note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. To support live migration, three or more hosts in the Harvester cluster are required due to a known issue . Starting a Migration \u00b6 Go to the Virtual Machines page. Find the virtual machine that you want to migrate and select \u22ee > Migrate . Choose the node to which you want to migrate the virtual machine. Click Apply . Aborting a Migration \u00b6 Go to the Virtual Machines page. Find the virtual machine in migrating status that you want to abort. Select \u22ee > Abort Migration . Migration Timeouts \u00b6 Completion Timeout \u00b6 The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds. Progress Timeout \u00b6 Live migration will also be aborted when copying memory doesn't make any progress in 150s.","title":"Live Migration"},{"location":"vm/live-migration/#live-migration","text":"Live migration means moving a virtual machine to a different host without downtime. Note Live migration is not allowed when the virtual machine is using a management network of bridge interface type. To support live migration, three or more hosts in the Harvester cluster are required due to a known issue .","title":"Live Migration"},{"location":"vm/live-migration/#starting-a-migration","text":"Go to the Virtual Machines page. Find the virtual machine that you want to migrate and select \u22ee > Migrate . Choose the node to which you want to migrate the virtual machine. Click Apply .","title":"Starting a Migration"},{"location":"vm/live-migration/#aborting-a-migration","text":"Go to the Virtual Machines page. Find the virtual machine in migrating status that you want to abort. Select \u22ee > Abort Migration .","title":"Aborting a Migration"},{"location":"vm/live-migration/#migration-timeouts","text":"","title":"Migration Timeouts"},{"location":"vm/live-migration/#completion-timeout","text":"The live migration process will copy virtual machine memory pages and disk blocks to the destination. In some cases, the virtual machine can write to different memory pages or disk blocks at a higher rate than these can be copied. As a result, the migration process is prevented from being completed in a reasonable amount of time. Live migration will be aborted if it exceeds the completion timeout of 800s per GiB of data. For example, a virtual machine with 8 GiB of memory will time out after 6400 seconds.","title":"Completion Timeout"},{"location":"vm/live-migration/#progress-timeout","text":"Live migration will also be aborted when copying memory doesn't make any progress in 150s.","title":"Progress Timeout"}]}